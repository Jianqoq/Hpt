// This file contains code ported from SLEEF (https://github.com/shibatch/sleef)
//
// Original work Copyright (c) 2010-2022, Naoki Shibata and contributors
// Modified work Copyright (c) 2024 hpt Contributors
//
// Boost Software License - Version 1.0 - August 17th, 2003
//
// Permission is hereby granted, free of charge, to any person or organization
// obtaining a copy of the software and accompanying documentation covered by
// this license (the "Software") to use, reproduce, display, distribute,
// execute, and transmit the Software, and to prepare derivative works of the
// Software, and to permit third-parties to whom the Software is furnished to
// do so, all subject to the following:
//
// The copyright notices in the Software and this entire statement, including
// the above license grant, this restriction and the following disclaimer,
// must be included in all copies of the Software, in whole or in part, and
// all derivative works of the Software, unless such copies or derivative
// works are solely in the form of machine-executable object code generated by
// a source language processor.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
// SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
// FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
// ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
// DEALINGS IN THE SOFTWARE.
//
// This Rust port is additionally licensed under Apache-2.0 OR MIT
// See repository root for details

#[cfg(all(target_arch = "aarch64", target_feature = "neon"))]
use crate::arch_simd::sleef::arch::helper_aarch64 as helper;
#[cfg(all(target_arch = "x86_64", target_feature = "avx2", not(target_feature = "avx512f")))]
use crate::arch_simd::sleef::arch::helper_avx2 as helper;
#[cfg(all(target_arch = "x86_64", target_feature = "avx512f"))]
use crate::arch_simd::sleef::arch::helper_avx512 as helper;
#[cfg(all(
    target_arch = "x86_64",
    target_feature = "sse",
    not(target_feature = "avx2")
))]
use crate::arch_simd::sleef::arch::helper_sse as helper;

use helper::{
    vabs_vd_vd, vadd_vd_vd_vd, vadd_vi_vi_vi, vand_vi_vi_vi, vand_vi_vo_vi, vand_vm_vm_vm,
    vand_vm_vo64_vm, vand_vo_vo_vo, vandnot_vi_vi_vi, vandnot_vm_vo64_vm, vandnot_vo_vo_vo,
    vcast_vd_d, vcast_vd_vi, vcast_vi_i, vcast_vm_i_i, vcast_vo32_vo64, vcast_vo64_vo32,
    veq_vo_vd_vd, veq_vo_vi_vi, vgather_vd_p_vi, vge_vo_vd_vd, vgt_vo_vd_vd, vgt_vo_vi_vi,
    visinf_vo_vd, visnan_vo_vd, vispinf_vo_vd, vle_vo_vd_vd, vlt_vo_vd_vd, vmax_vd_vd_vd,
    vmin_vd_vd_vd, vmla_vd_vd_vd_vd, vmlapn_vd_vd_vd_vd, vmul_vd_vd_vd, vneg_vd_vd, vneg_vi_vi,
    vor_vm_vm_vm, vor_vm_vo64_vm, vor_vo_vo_vo, vreinterpret_vd_vm, vreinterpret_vm_vd,
    vrint_vd_vd, vrint_vi_vd, vsel_vd_vo_d_d, vsel_vd_vo_vd_vd, vsel_vi_vo_vi_vi, vsll_vi_vi_i,
    vsra_vi_vi_i, vsrl64_vm_vm_i, vsub64_vm_vm_vm, vsub_vd_vd_vd, vsub_vi_vi_vi,
    vtestallones_i_vo64, vtruncate_vd_vd, vtruncate_vi_vd, vxor_vm_vm_vm, vxor_vo_vo_vo,
};

use crate::{
    arch_simd::sleef::{
        common::{
            commonfuncs::{
                ddigetdd_vd2_ddi, ddigeti_vi_ddi, ddisetdd_ddi_ddi_vd2, ddisetddi_ddi_vd2_vi,
                digetd_vd_di, digeti_vi_di, rempisub, vilogb2k_vi_vd, vilogbk_vi_vd, visint_vo_vd,
                visnegzero_vo_vd, visodd_vo_vd, vldexp2_vd_vd_vi, vldexp3_vd_vd_vi,
                vmulsign_vd_vd_vd, vround2_vd_vd, vsignbit_vm_vd, vsignbit_vo_vd, vtruncate2_vd_vd,
                DDI,
            },
            dd::{
                ddadd2_vd2_vd2_vd, ddadd2_vd2_vd2_vd2, ddadd2_vd2_vd_vd, ddadd2_vd2_vd_vd2,
                ddadd_vd2_vd2_vd, ddadd_vd2_vd2_vd2, ddadd_vd2_vd_vd, ddadd_vd2_vd_vd2,
                dddiv_vd2_vd2_vd2, ddmul_vd2_vd2_vd, ddmul_vd2_vd2_vd2, ddmul_vd2_vd_vd,
                ddmul_vd_vd2_vd2, ddneg_vd2_vd2, ddnormalize_vd2_vd2, ddrec_vd2_vd, ddrec_vd2_vd2,
                ddscale_vd2_vd2_vd, ddsqrt_vd2_vd, ddsqrt_vd2_vd2, ddsqu_vd2_vd2, ddsqu_vd_vd2,
                ddsub_vd2_vd2_vd, ddsub_vd2_vd2_vd2, vcast_vd2_d_d, vcast_vd2_vd_vd,
                vd2getx_vd_vd2, vd2gety_vd_vd2, vd2setx_vd2_vd2_vd, vd2setxy_vd2_vd_vd,
                vd2sety_vd2_vd2_vd, vsel_vd2_vo_vd2_vd2, VDouble2,
            },
            estrin::{
                poly10d, poly12d, poly16d, poly21d, poly21d_, poly6d, poly7d, poly8d, poly9d,
            },
            misc::{
                L10_L, L10_U, L2L, L2U, LOG10_2, LOG1P_BOUND, LOG_DBL_MAX, M_1_PI, M_2_PI_H,
                M_2_PI_L, M_PI, PI_A, PI_A2, PI_B, PI_B2, PI_C, PI_D, R_LN2, SLEEF_DBL_MIN,
                SQRT_DBL_MAX, TRIGRANGEMAX, TRIGRANGEMAX2,
            },
        },
        table::SLEEF_REMPITABDP,
    },
    simd::sleef::common::commonfuncs::vcopysign_vd_vd_vd,
    sleef_types::{VDouble, VInt, Vopmask},
};

#[inline(always)]
unsafe fn vsel_vi_vd_vd_vi_vi(d0: VDouble, d1: VDouble, x: VInt, y: VInt) -> VInt {
    vsel_vi_vo_vi_vi(vcast_vo32_vo64(vlt_vo_vd_vd(d0, d1)), x, y)
}

#[inline(always)]
unsafe fn vsel_vi_vd_vi(d: VDouble, x: VInt) -> VInt {
    vand_vi_vo_vi(vcast_vo32_vo64(vsignbit_vo_vd(d)), x)
}

#[inline(always)]
unsafe fn rempi(a: VDouble) -> DDI {
    let mut x: VDouble2;
    let mut y: VDouble2;

    let mut ex = vilogb2k_vi_vd(a);

    #[cfg(target_feature = "avx512f")]
    {
        ex = vandnot_vi_vi_vi(vsra_vi_vi_i::<31>(ex), ex);
        ex = vand_vi_vi_vi(ex, vcast_vi_i(1023));
    }

    ex = vsub_vi_vi_vi(ex, vcast_vi_i(55));

    let q = vand_vi_vo_vi(vgt_vo_vi_vi(ex, vcast_vi_i(700 - 55)), vcast_vi_i(-64));
    let a = vldexp3_vd_vd_vi(a, q);

    ex = vandnot_vi_vi_vi(vsra_vi_vi_i::<31>(ex), ex);
    ex = vsll_vi_vi_i::<2>(ex);

    x = ddmul_vd2_vd_vd(a, vgather_vd_p_vi(SLEEF_REMPITABDP.as_ptr(), ex));
    let mut di = rempisub(vd2getx_vd_vd2(x));
    let mut q = digeti_vi_di(di);
    x = vd2setx_vd2_vd2_vd(x, digetd_vd_di(di));
    x = ddnormalize_vd2_vd2(x);

    y = ddmul_vd2_vd_vd(a, vgather_vd_p_vi(SLEEF_REMPITABDP.as_ptr().add(1), ex));
    x = ddadd2_vd2_vd2_vd2(x, y);
    di = rempisub(vd2getx_vd_vd2(x));
    q = vadd_vi_vi_vi(q, digeti_vi_di(di));
    x = vd2setx_vd2_vd2_vd(x, digetd_vd_di(di));
    x = ddnormalize_vd2_vd2(x);

    y = vcast_vd2_vd_vd(
        vgather_vd_p_vi(SLEEF_REMPITABDP.as_ptr().add(2), ex),
        vgather_vd_p_vi(SLEEF_REMPITABDP.as_ptr().add(3), ex),
    );
    y = ddmul_vd2_vd2_vd(y, a);
    x = ddadd2_vd2_vd2_vd2(x, y);
    x = ddnormalize_vd2_vd2(x);

    x = ddmul_vd2_vd2_vd2(
        x,
        vcast_vd2_d_d(
            3.141_592_653_589_793 * 2.0,
            1.224_646_799_147_353_2e-16 * 2.0,
        ),
    );

    let o = vlt_vo_vd_vd(vabs_vd_vd(a), vcast_vd_d(0.7));
    x = vd2setx_vd2_vd2_vd(x, vsel_vd_vo_vd_vd(o, a, vd2getx_vd_vd2(x)));
    x = vd2sety_vd2_vd2_vd(
        x,
        vreinterpret_vd_vm(vandnot_vm_vo64_vm(o, vreinterpret_vm_vd(vd2gety_vd_vd2(x)))),
    );

    ddisetddi_ddi_vd2_vi(x, q)
}

#[inline(always)]
pub(crate) unsafe fn xsin_u1(d: VDouble) -> VDouble {
    let mut u: VDouble;
    let mut s: VDouble2;

    let mut x: VDouble2;
    let mut ql: VInt;

    let mut g = vlt_vo_vd_vd(vabs_vd_vd(d), vcast_vd_d(TRIGRANGEMAX2));
    let dql = vrint_vd_vd(vmul_vd_vd_vd(d, vcast_vd_d(M_1_PI)));
    ql = vrint_vi_vd(dql);
    u = vmla_vd_vd_vd_vd(dql, vcast_vd_d(-PI_A2), d);
    x = ddadd_vd2_vd_vd(u, vmul_vd_vd_vd(dql, vcast_vd_d(-PI_B2)));

    if vtestallones_i_vo64(g) == 0 {
        let mut dqh = vtruncate_vd_vd(vmul_vd_vd_vd(d, vcast_vd_d(M_1_PI / (1 << 24) as f64)));
        dqh = vmul_vd_vd_vd(dqh, vcast_vd_d((1 << 24) as f64));
        let dql = vrint_vd_vd(vmlapn_vd_vd_vd_vd(d, vcast_vd_d(M_1_PI), dqh));

        u = vmla_vd_vd_vd_vd(dqh, vcast_vd_d(-PI_A), d);
        s = ddadd_vd2_vd_vd(u, vmul_vd_vd_vd(dql, vcast_vd_d(-PI_A)));
        s = ddadd2_vd2_vd2_vd(s, vmul_vd_vd_vd(dqh, vcast_vd_d(-PI_B)));
        s = ddadd2_vd2_vd2_vd(s, vmul_vd_vd_vd(dql, vcast_vd_d(-PI_B)));
        s = ddadd2_vd2_vd2_vd(s, vmul_vd_vd_vd(dqh, vcast_vd_d(-PI_C)));
        s = ddadd2_vd2_vd2_vd(s, vmul_vd_vd_vd(dql, vcast_vd_d(-PI_C)));
        s = ddadd_vd2_vd2_vd(s, vmul_vd_vd_vd(vadd_vd_vd_vd(dqh, dql), vcast_vd_d(-PI_D)));

        ql = vsel_vi_vo_vi_vi(vcast_vo32_vo64(g), ql, vrint_vi_vd(dql));
        x = vsel_vd2_vo_vd2_vd2(g, x, s);
        g = vlt_vo_vd_vd(vabs_vd_vd(d), vcast_vd_d(TRIGRANGEMAX));

        if vtestallones_i_vo64(g) == 0 {
            let ddi = rempi(d);
            let mut ql2 = vand_vi_vi_vi(ddigeti_vi_ddi(ddi), vcast_vi_i(3));
            ql2 = vadd_vi_vi_vi(
                vadd_vi_vi_vi(ql2, ql2),
                vsel_vi_vo_vi_vi(
                    vcast_vo32_vo64(vgt_vo_vd_vd(
                        vd2getx_vd_vd2(ddigetdd_vd2_ddi(ddi)),
                        vcast_vd_d(0.0),
                    )),
                    vcast_vi_i(2),
                    vcast_vi_i(1),
                ),
            );
            ql2 = vsra_vi_vi_i::<2>(ql2);

            let o = veq_vo_vi_vi(
                vand_vi_vi_vi(ddigeti_vi_ddi(ddi), vcast_vi_i(1)),
                vcast_vi_i(1),
            );

            let mut t = vcast_vd2_vd_vd(
                vmulsign_vd_vd_vd(
                    vcast_vd_d(-3.141_592_653_589_793 * 0.5),
                    vd2getx_vd_vd2(ddigetdd_vd2_ddi(ddi)),
                ),
                vmulsign_vd_vd_vd(
                    vcast_vd_d(-1.224_646_799_147_353_2e-16 * 0.5),
                    vd2getx_vd_vd2(ddigetdd_vd2_ddi(ddi)),
                ),
            );

            t = ddadd2_vd2_vd2_vd2(ddigetdd_vd2_ddi(ddi), t);
            let ddi = ddisetdd_ddi_ddi_vd2(
                ddi,
                vsel_vd2_vo_vd2_vd2(vcast_vo64_vo32(o), t, ddigetdd_vd2_ddi(ddi)),
            );
            s = ddnormalize_vd2_vd2(ddigetdd_vd2_ddi(ddi));
            ql = vsel_vi_vo_vi_vi(vcast_vo32_vo64(g), ql, ql2);
            x = vsel_vd2_vo_vd2_vd2(g, x, s);
            x = vd2setx_vd2_vd2_vd(
                x,
                vreinterpret_vd_vm(vor_vm_vo64_vm(
                    vor_vo_vo_vo(visinf_vo_vd(d), visnan_vo_vd(d)),
                    vreinterpret_vm_vd(vd2getx_vd_vd2(x)),
                )),
            );
        }
    }

    let t: VDouble2 = x;
    s = ddsqu_vd2_vd2(x);

    let s2 = vmul_vd_vd_vd(vd2getx_vd_vd2(s), vd2getx_vd_vd2(s));
    let s4 = vmul_vd_vd_vd(s2, s2);

    u = poly6d(
        vd2getx_vd_vd2(s),
        s2,
        s4,
        2.720_524_161_385_295_7e-15,
        -7.642_925_941_139_545e-13,
        1.605_893_701_172_779e-10,
        -2.505_210_681_484_312_3e-8,
        2.755_731_921_044_282_2e-6,
        -0.000_198_412_698_412_046_45,
    );

    u = vmla_vd_vd_vd_vd(u, vd2getx_vd_vd2(s), vcast_vd_d(0.008_333_333_333_333_18));

    x = ddadd_vd2_vd_vd2(
        vcast_vd_d(1.0),
        ddmul_vd2_vd2_vd2(
            ddadd_vd2_vd_vd(
                vcast_vd_d(-0.166_666_666_666_666_66),
                vmul_vd_vd_vd(u, vd2getx_vd_vd2(s)),
            ),
            s,
        ),
    );
    u = ddmul_vd_vd2_vd2(t, x);

    u = vreinterpret_vd_vm(vxor_vm_vm_vm(
        vand_vm_vo64_vm(
            vcast_vo64_vo32(veq_vo_vi_vi(
                vand_vi_vi_vi(ql, vcast_vi_i(1)),
                vcast_vi_i(1),
            )),
            vreinterpret_vm_vd(vcast_vd_d(-0.0)),
        ),
        vreinterpret_vm_vd(u),
    ));

    vsel_vd_vo_vd_vd(veq_vo_vd_vd(d, vcast_vd_d(0.0)), d, u)
}

#[inline(always)]
pub(crate) unsafe fn xcos_u1(d: VDouble) -> VDouble {
    let mut u: VDouble;
    let mut s: VDouble2;

    let mut x: VDouble2;
    let mut ql: VInt;

    let mut g = vlt_vo_vd_vd(vabs_vd_vd(d), vcast_vd_d(TRIGRANGEMAX2));
    let mut dql = vrint_vd_vd(vmla_vd_vd_vd_vd(d, vcast_vd_d(M_1_PI), vcast_vd_d(-0.5)));
    dql = vmla_vd_vd_vd_vd(vcast_vd_d(2.0), dql, vcast_vd_d(1.0));
    ql = vrint_vi_vd(dql);
    x = ddadd2_vd2_vd_vd(d, vmul_vd_vd_vd(dql, vcast_vd_d(-PI_A2 * 0.5)));
    x = ddadd_vd2_vd2_vd(x, vmul_vd_vd_vd(dql, vcast_vd_d(-PI_B2 * 0.5)));

    if vtestallones_i_vo64(g) == 0 {
        let mut dqh = vtruncate_vd_vd(vmla_vd_vd_vd_vd(
            d,
            vcast_vd_d(M_1_PI / (1 << 23) as f64),
            vcast_vd_d(-M_1_PI / (1 << 24) as f64),
        ));
        let mut ql2 = vrint_vi_vd(vadd_vd_vd_vd(
            vmul_vd_vd_vd(d, vcast_vd_d(M_1_PI)),
            vmla_vd_vd_vd_vd(dqh, vcast_vd_d(-(1 << 23) as f64), vcast_vd_d(-0.5)),
        ));
        dqh = vmul_vd_vd_vd(dqh, vcast_vd_d((1 << 24) as f64));
        ql2 = vadd_vi_vi_vi(vadd_vi_vi_vi(ql2, ql2), vcast_vi_i(1));
        let dql = vcast_vd_vi(ql2);

        u = vmla_vd_vd_vd_vd(dqh, vcast_vd_d(-PI_A * 0.5), d);
        s = ddadd2_vd2_vd_vd(u, vmul_vd_vd_vd(dql, vcast_vd_d(-PI_A * 0.5)));
        s = ddadd2_vd2_vd2_vd(s, vmul_vd_vd_vd(dqh, vcast_vd_d(-PI_B * 0.5)));
        s = ddadd2_vd2_vd2_vd(s, vmul_vd_vd_vd(dql, vcast_vd_d(-PI_B * 0.5)));
        s = ddadd2_vd2_vd2_vd(s, vmul_vd_vd_vd(dqh, vcast_vd_d(-PI_C * 0.5)));
        s = ddadd2_vd2_vd2_vd(s, vmul_vd_vd_vd(dql, vcast_vd_d(-PI_C * 0.5)));
        s = ddadd_vd2_vd2_vd(
            s,
            vmul_vd_vd_vd(vadd_vd_vd_vd(dqh, dql), vcast_vd_d(-PI_D * 0.5)),
        );

        ql = vsel_vi_vo_vi_vi(vcast_vo32_vo64(g), ql, ql2);
        x = vsel_vd2_vo_vd2_vd2(g, x, s);
        g = vlt_vo_vd_vd(vabs_vd_vd(d), vcast_vd_d(TRIGRANGEMAX));

        if vtestallones_i_vo64(g) == 0 {
            let ddi = rempi(d);
            let mut ql2 = vand_vi_vi_vi(ddigeti_vi_ddi(ddi), vcast_vi_i(3));
            ql2 = vadd_vi_vi_vi(
                vadd_vi_vi_vi(ql2, ql2),
                vsel_vi_vo_vi_vi(
                    vcast_vo32_vo64(vgt_vo_vd_vd(
                        vd2getx_vd_vd2(ddigetdd_vd2_ddi(ddi)),
                        vcast_vd_d(0.0),
                    )),
                    vcast_vi_i(8),
                    vcast_vi_i(7),
                ),
            );
            ql2 = vsra_vi_vi_i::<1>(ql2);

            let o = veq_vo_vi_vi(
                vand_vi_vi_vi(ddigeti_vi_ddi(ddi), vcast_vi_i(1)),
                vcast_vi_i(0),
            );

            let y = vsel_vd_vo_vd_vd(
                vgt_vo_vd_vd(vd2getx_vd_vd2(ddigetdd_vd2_ddi(ddi)), vcast_vd_d(0.0)),
                vcast_vd_d(0.0),
                vcast_vd_d(-1.0),
            );

            let mut t = vcast_vd2_vd_vd(
                vmulsign_vd_vd_vd(vcast_vd_d(-3.141_592_653_589_793 * 0.5), y),
                vmulsign_vd_vd_vd(vcast_vd_d(-1.224_646_799_147_353_2e-16 * 0.5), y),
            );

            t = ddadd2_vd2_vd2_vd2(ddigetdd_vd2_ddi(ddi), t);
            let ddi = ddisetdd_ddi_ddi_vd2(
                ddi,
                vsel_vd2_vo_vd2_vd2(vcast_vo64_vo32(o), t, ddigetdd_vd2_ddi(ddi)),
            );
            s = ddnormalize_vd2_vd2(ddigetdd_vd2_ddi(ddi));
            ql = vsel_vi_vo_vi_vi(vcast_vo32_vo64(g), ql, ql2);
            x = vsel_vd2_vo_vd2_vd2(g, x, s);
            x = vd2setx_vd2_vd2_vd(
                x,
                vreinterpret_vd_vm(vor_vm_vo64_vm(
                    vor_vo_vo_vo(visinf_vo_vd(d), visnan_vo_vd(d)),
                    vreinterpret_vm_vd(vd2getx_vd_vd2(x)),
                )),
            );
        }
    }

    let t: VDouble2 = x;
    s = ddsqu_vd2_vd2(x);

    let s2 = vmul_vd_vd_vd(vd2getx_vd_vd2(s), vd2getx_vd_vd2(s));
    let s4 = vmul_vd_vd_vd(s2, s2);

    u = poly6d(
        vd2getx_vd_vd2(s),
        s2,
        s4,
        2.720_524_161_385_295_7e-15,
        -7.642_925_941_139_545e-13,
        1.605_893_701_172_779e-10,
        -2.505_210_681_484_312_3e-8,
        2.755_731_921_044_282_2e-6,
        -0.000_198_412_698_412_046_45,
    );

    u = vmla_vd_vd_vd_vd(u, vd2getx_vd_vd2(s), vcast_vd_d(0.008_333_333_333_333_18));

    x = ddadd_vd2_vd_vd2(
        vcast_vd_d(1.0),
        ddmul_vd2_vd2_vd2(
            ddadd_vd2_vd_vd(
                vcast_vd_d(-0.166_666_666_666_666_66),
                vmul_vd_vd_vd(u, vd2getx_vd_vd2(s)),
            ),
            s,
        ),
    );
    u = ddmul_vd_vd2_vd2(t, x);

    u = vreinterpret_vd_vm(vxor_vm_vm_vm(
        vand_vm_vo64_vm(
            vcast_vo64_vo32(veq_vo_vi_vi(
                vand_vi_vi_vi(ql, vcast_vi_i(2)),
                vcast_vi_i(0),
            )),
            vreinterpret_vm_vd(vcast_vd_d(-0.0)),
        ),
        vreinterpret_vm_vd(u),
    ));

    u
}

#[inline(always)]
pub(crate) unsafe fn xsincos_u1(d: VDouble) -> VDouble2 {
    let mut o: Vopmask;
    let mut u: VDouble;
    let mut rx: VDouble;

    let mut r: VDouble2;
    let mut s: VDouble2;

    let mut x: VDouble2;
    let mut ql: VInt;

    let dql = vrint_vd_vd(vmul_vd_vd_vd(d, vcast_vd_d(2.0 * M_1_PI)));
    ql = vrint_vi_vd(dql);
    u = vmla_vd_vd_vd_vd(dql, vcast_vd_d(-PI_A2 * 0.5), d);
    s = ddadd_vd2_vd_vd(u, vmul_vd_vd_vd(dql, vcast_vd_d(-PI_B2 * 0.5)));
    let mut g = vlt_vo_vd_vd(vabs_vd_vd(d), vcast_vd_d(TRIGRANGEMAX2));

    if vtestallones_i_vo64(g) == 0 {
        let mut dqh = vtruncate_vd_vd(vmul_vd_vd_vd(
            d,
            vcast_vd_d(2.0 * M_1_PI / (1 << 24) as f64),
        ));
        dqh = vmul_vd_vd_vd(dqh, vcast_vd_d((1 << 24) as f64));
        let dql = vrint_vd_vd(vsub_vd_vd_vd(
            vmul_vd_vd_vd(d, vcast_vd_d(2.0 * M_1_PI)),
            dqh,
        ));

        u = vmla_vd_vd_vd_vd(dqh, vcast_vd_d(-PI_A * 0.5), d);
        x = ddadd_vd2_vd_vd(u, vmul_vd_vd_vd(dql, vcast_vd_d(-PI_A * 0.5)));
        x = ddadd2_vd2_vd2_vd(x, vmul_vd_vd_vd(dqh, vcast_vd_d(-PI_B * 0.5)));
        x = ddadd2_vd2_vd2_vd(x, vmul_vd_vd_vd(dql, vcast_vd_d(-PI_B * 0.5)));
        x = ddadd2_vd2_vd2_vd(x, vmul_vd_vd_vd(dqh, vcast_vd_d(-PI_C * 0.5)));
        x = ddadd2_vd2_vd2_vd(x, vmul_vd_vd_vd(dql, vcast_vd_d(-PI_C * 0.5)));
        x = ddadd_vd2_vd2_vd(
            x,
            vmul_vd_vd_vd(vadd_vd_vd_vd(dqh, dql), vcast_vd_d(-PI_D * 0.5)),
        );

        ql = vsel_vi_vo_vi_vi(vcast_vo32_vo64(g), ql, vrint_vi_vd(dql));
        s = vsel_vd2_vo_vd2_vd2(g, s, x);
        g = vlt_vo_vd_vd(vabs_vd_vd(d), vcast_vd_d(TRIGRANGEMAX));

        if vtestallones_i_vo64(g) == 0 {
            let ddi = rempi(d);
            x = ddigetdd_vd2_ddi(ddi);
            o = vor_vo_vo_vo(visinf_vo_vd(d), visnan_vo_vd(d));
            x = vd2setx_vd2_vd2_vd(
                x,
                vreinterpret_vd_vm(vor_vm_vo64_vm(o, vreinterpret_vm_vd(vd2getx_vd_vd2(x)))),
            );
            x = vd2sety_vd2_vd2_vd(
                x,
                vreinterpret_vd_vm(vor_vm_vo64_vm(o, vreinterpret_vm_vd(vd2gety_vd_vd2(x)))),
            );

            ql = vsel_vi_vo_vi_vi(vcast_vo32_vo64(g), ql, ddigeti_vi_ddi(ddi));
            s = vsel_vd2_vo_vd2_vd2(g, s, x);
        }
    }

    let t: VDouble2 = s;
    s = vd2setx_vd2_vd2_vd(s, ddsqu_vd_vd2(s));

    u = vcast_vd_d(1.589_383_072_832_289_5e-10);
    u = vmla_vd_vd_vd_vd(u, vd2getx_vd_vd2(s), vcast_vd_d(-2.505_069_435_025_398e-8));
    u = vmla_vd_vd_vd_vd(u, vd2getx_vd_vd2(s), vcast_vd_d(2.755_731_317_768_463_6e-6));
    u = vmla_vd_vd_vd_vd(
        u,
        vd2getx_vd_vd2(s),
        vcast_vd_d(-0.000_198_412_698_278_911_77),
    );
    u = vmla_vd_vd_vd_vd(u, vd2getx_vd_vd2(s), vcast_vd_d(0.008_333_333_333_319_185));
    u = vmla_vd_vd_vd_vd(u, vd2getx_vd_vd2(s), vcast_vd_d(-0.166_666_666_666_666_13));

    u = vmul_vd_vd_vd(u, vmul_vd_vd_vd(vd2getx_vd_vd2(s), vd2getx_vd_vd2(t)));

    x = ddadd_vd2_vd2_vd(t, u);
    rx = vadd_vd_vd_vd(vd2getx_vd_vd2(x), vd2gety_vd_vd2(x));

    rx = vsel_vd_vo_vd_vd(visnegzero_vo_vd(d), vcast_vd_d(-0.0), rx);

    u = vcast_vd_d(-1.136_153_502_390_974_4e-11);
    u = vmla_vd_vd_vd_vd(u, vd2getx_vd_vd2(s), vcast_vd_d(2.087_574_712_070_400_6e-9));
    u = vmla_vd_vd_vd_vd(
        u,
        vd2getx_vd_vd2(s),
        vcast_vd_d(-2.755_731_440_288_475_5e-7),
    );
    u = vmla_vd_vd_vd_vd(u, vd2getx_vd_vd2(s), vcast_vd_d(2.480_158_728_900_018_5e-5));
    u = vmla_vd_vd_vd_vd(
        u,
        vd2getx_vd_vd2(s),
        vcast_vd_d(-0.001_388_888_888_887_140_1),
    );
    u = vmla_vd_vd_vd_vd(u, vd2getx_vd_vd2(s), vcast_vd_d(0.041_666_666_666_666_55));
    u = vmla_vd_vd_vd_vd(u, vd2getx_vd_vd2(s), vcast_vd_d(-0.5));

    x = ddadd_vd2_vd_vd2(vcast_vd_d(1.0), ddmul_vd2_vd_vd(vd2getx_vd_vd2(s), u));
    let ry: VDouble = vadd_vd_vd_vd(vd2getx_vd_vd2(x), vd2gety_vd_vd2(x));

    o = vcast_vo64_vo32(veq_vo_vi_vi(
        vand_vi_vi_vi(ql, vcast_vi_i(1)),
        vcast_vi_i(0),
    ));
    r = vd2setxy_vd2_vd_vd(vsel_vd_vo_vd_vd(o, rx, ry), vsel_vd_vo_vd_vd(o, ry, rx));

    o = vcast_vo64_vo32(veq_vo_vi_vi(
        vand_vi_vi_vi(ql, vcast_vi_i(2)),
        vcast_vi_i(2),
    ));
    r = vd2setx_vd2_vd2_vd(
        r,
        vreinterpret_vd_vm(vxor_vm_vm_vm(
            vand_vm_vo64_vm(o, vreinterpret_vm_vd(vcast_vd_d(-0.0))),
            vreinterpret_vm_vd(vd2getx_vd_vd2(r)),
        )),
    );

    o = vcast_vo64_vo32(veq_vo_vi_vi(
        vand_vi_vi_vi(vadd_vi_vi_vi(ql, vcast_vi_i(1)), vcast_vi_i(2)),
        vcast_vi_i(2),
    ));
    r = vd2sety_vd2_vd2_vd(
        r,
        vreinterpret_vd_vm(vxor_vm_vm_vm(
            vand_vm_vo64_vm(o, vreinterpret_vm_vd(vcast_vd_d(-0.0))),
            vreinterpret_vm_vd(vd2gety_vd_vd2(r)),
        )),
    );

    r
}

#[inline(always)]
pub(crate) unsafe fn xtan_u1(d: VDouble) -> VDouble {
    let mut u: VDouble;
    let mut s: VDouble2;

    let mut x: VDouble2;

    let mut o: Vopmask;
    let mut ql: VInt;

    let dql = vrint_vd_vd(vmul_vd_vd_vd(d, vcast_vd_d(2.0 * M_1_PI)));
    ql = vrint_vi_vd(dql);
    u = vmla_vd_vd_vd_vd(dql, vcast_vd_d(-PI_A2 * 0.5), d);
    s = ddadd_vd2_vd_vd(u, vmul_vd_vd_vd(dql, vcast_vd_d(-PI_B2 * 0.5)));
    let mut g = vlt_vo_vd_vd(vabs_vd_vd(d), vcast_vd_d(TRIGRANGEMAX2));

    if vtestallones_i_vo64(g) == 0 {
        let mut dqh = vtruncate_vd_vd(vmul_vd_vd_vd(
            d,
            vcast_vd_d(2.0 * M_1_PI / (1 << 24) as f64),
        ));
        dqh = vmul_vd_vd_vd(dqh, vcast_vd_d((1 << 24) as f64));
        x = ddadd2_vd2_vd2_vd(
            ddmul_vd2_vd2_vd(vcast_vd2_d_d(M_2_PI_H, M_2_PI_L), d),
            vsub_vd_vd_vd(
                vsel_vd_vo_vd_vd(
                    vlt_vo_vd_vd(d, vcast_vd_d(0.0)),
                    vcast_vd_d(-0.5),
                    vcast_vd_d(0.5),
                ),
                dqh,
            ),
        );
        let dql = vtruncate_vd_vd(vadd_vd_vd_vd(vd2getx_vd_vd2(x), vd2gety_vd_vd2(x)));

        u = vmla_vd_vd_vd_vd(dqh, vcast_vd_d(-PI_A * 0.5), d);
        x = ddadd_vd2_vd_vd(u, vmul_vd_vd_vd(dql, vcast_vd_d(-PI_A * 0.5)));
        x = ddadd2_vd2_vd2_vd(x, vmul_vd_vd_vd(dqh, vcast_vd_d(-PI_B * 0.5)));
        x = ddadd2_vd2_vd2_vd(x, vmul_vd_vd_vd(dql, vcast_vd_d(-PI_B * 0.5)));
        x = ddadd2_vd2_vd2_vd(x, vmul_vd_vd_vd(dqh, vcast_vd_d(-PI_C * 0.5)));
        x = ddadd2_vd2_vd2_vd(x, vmul_vd_vd_vd(dql, vcast_vd_d(-PI_C * 0.5)));
        x = ddadd_vd2_vd2_vd(
            x,
            vmul_vd_vd_vd(vadd_vd_vd_vd(dqh, dql), vcast_vd_d(-PI_D * 0.5)),
        );

        ql = vsel_vi_vo_vi_vi(vcast_vo32_vo64(g), ql, vrint_vi_vd(dql));
        s = vsel_vd2_vo_vd2_vd2(g, s, x);
        g = vlt_vo_vd_vd(vabs_vd_vd(d), vcast_vd_d(TRIGRANGEMAX));

        if vtestallones_i_vo64(g) == 0 {
            let ddi = rempi(d);
            x = ddigetdd_vd2_ddi(ddi);
            o = vor_vo_vo_vo(visinf_vo_vd(d), visnan_vo_vd(d));
            x = vd2setx_vd2_vd2_vd(
                x,
                vreinterpret_vd_vm(vor_vm_vo64_vm(o, vreinterpret_vm_vd(vd2getx_vd_vd2(x)))),
            );
            x = vd2sety_vd2_vd2_vd(
                x,
                vreinterpret_vd_vm(vor_vm_vo64_vm(o, vreinterpret_vm_vd(vd2gety_vd_vd2(x)))),
            );

            ql = vsel_vi_vo_vi_vi(vcast_vo32_vo64(g), ql, ddigeti_vi_ddi(ddi));
            s = vsel_vd2_vo_vd2_vd2(g, s, x);
        }
    }

    let t: VDouble2 = ddscale_vd2_vd2_vd(s, vcast_vd_d(0.5));
    s = ddsqu_vd2_vd2(t);

    let s2 = vmul_vd_vd_vd(vd2getx_vd_vd2(s), vd2getx_vd_vd2(s));
    let s4 = vmul_vd_vd_vd(s2, s2);

    u = poly8d(
        vd2getx_vd_vd2(s),
        s2,
        s4,
        3.245_098_826_639_276_3e-4,
        5.619_219_738_114_324e-4,
        1.460_781_502_402_784_5e-3,
        3.591_611_540_792_499_5e-3,
        8.863_268_409_563_113e-3,
        2.186_948_728_185_535_5e-2,
        5.396_825_399_517_273e-2,
        1.333_333_333_330_500_6e-1,
    );

    u = vmla_vd_vd_vd_vd(u, vd2getx_vd_vd2(s), vcast_vd_d(3.333_333_333_333_343_7e-1));
    x = ddadd_vd2_vd2_vd2(t, ddmul_vd2_vd2_vd(ddmul_vd2_vd2_vd2(s, t), u));

    let y: VDouble2 = ddadd_vd2_vd_vd2(vcast_vd_d(-1.0), ddsqu_vd2_vd2(x));
    x = ddscale_vd2_vd2_vd(x, vcast_vd_d(-2.0));

    o = vcast_vo64_vo32(veq_vo_vi_vi(
        vand_vi_vi_vi(ql, vcast_vi_i(1)),
        vcast_vi_i(1),
    ));

    x = dddiv_vd2_vd2_vd2(
        vsel_vd2_vo_vd2_vd2(o, ddneg_vd2_vd2(y), x),
        vsel_vd2_vo_vd2_vd2(o, x, y),
    );

    u = vadd_vd_vd_vd(vd2getx_vd_vd2(x), vd2gety_vd_vd2(x));

    vsel_vd_vo_vd_vd(veq_vo_vd_vd(d, vcast_vd_d(0.0)), d, u)
}

#[inline(always)]
pub(crate) unsafe fn atan2k_u1(y: VDouble2, x: VDouble2) -> VDouble2 {
    let mut u: VDouble;
    let mut s: VDouble2;
    let mut t: VDouble2;
    let mut q: VInt;
    let mut p: Vopmask;

    q = vsel_vi_vd_vi(vd2getx_vd_vd2(x), vcast_vi_i(-2));
    p = vlt_vo_vd_vd(vd2getx_vd_vd2(x), vcast_vd_d(0.0));
    let b = vand_vm_vo64_vm(p, vreinterpret_vm_vd(vcast_vd_d(-0.0)));
    let x = vd2setx_vd2_vd2_vd(
        x,
        vreinterpret_vd_vm(vxor_vm_vm_vm(b, vreinterpret_vm_vd(vd2getx_vd_vd2(x)))),
    );
    let x = vd2sety_vd2_vd2_vd(
        x,
        vreinterpret_vd_vm(vxor_vm_vm_vm(b, vreinterpret_vm_vd(vd2gety_vd_vd2(x)))),
    );

    q = vsel_vi_vd_vd_vi_vi(
        vd2getx_vd_vd2(x),
        vd2getx_vd_vd2(y),
        vadd_vi_vi_vi(q, vcast_vi_i(1)),
        q,
    );
    p = vlt_vo_vd_vd(vd2getx_vd_vd2(x), vd2getx_vd_vd2(y));
    s = vsel_vd2_vo_vd2_vd2(p, ddneg_vd2_vd2(x), y);
    t = vsel_vd2_vo_vd2_vd2(p, y, x);

    s = dddiv_vd2_vd2_vd2(s, t);
    t = ddsqu_vd2_vd2(s);
    t = ddnormalize_vd2_vd2(t);

    let t2 = vmul_vd_vd_vd(vd2getx_vd_vd2(t), vd2getx_vd_vd2(t));
    let t4 = vmul_vd_vd_vd(t2, t2);
    let t8 = vmul_vd_vd_vd(t4, t4);

    u = poly16d(
        vd2getx_vd_vd2(t),
        t2,
        t4,
        t8,
        1.062_984_841_914_487_5e-5,
        -0.000_125_620_649_967_286_87,
        0.000_705_576_642_963_934_1,
        -0.002_518_656_144_987_133_6,
        0.006_462_628_990_369_912,
        -0.012_828_133_366_339_903,
        0.020_802_479_992_414_58,
        -0.028_900_234_478_474_03,
        0.035_978_500_503_510_46,
        -0.041_848_579_703_592_51,
        0.047_084_301_165_328_4,
        -0.052_491_421_058_844_84,
        0.058_794_659_096_958_1,
        -0.066_662_088_477_879_55,
        0.076_922_533_029_620_38,
        -0.090_909_044_277_338_76,
    );

    u = vmla_vd_vd_vd_vd(u, vd2getx_vd_vd2(t), vcast_vd_d(0.111_111_108_376_896_24));
    u = vmla_vd_vd_vd_vd(u, vd2getx_vd_vd2(t), vcast_vd_d(-0.142_857_142_756_268_57));
    u = vmla_vd_vd_vd_vd(u, vd2getx_vd_vd2(t), vcast_vd_d(0.199_999_999_997_977_35));
    u = vmla_vd_vd_vd_vd(u, vd2getx_vd_vd2(t), vcast_vd_d(-0.333_333_333_333_317_6));

    t = ddadd_vd2_vd2_vd2(s, ddmul_vd2_vd2_vd(ddmul_vd2_vd2_vd2(s, t), u));

    t = ddadd_vd2_vd2_vd2(
        ddmul_vd2_vd2_vd(
            vcast_vd2_d_d(1.570_796_326_794_896_6, 6.123_233_995_736_766e-17),
            vcast_vd_vi(q),
        ),
        t,
    );

    t
}

#[inline(always)]
pub(crate) unsafe fn visinf2_vd_vd_vd(d: VDouble, m: VDouble) -> VDouble {
    vreinterpret_vd_vm(vand_vm_vo64_vm(
        visinf_vo_vd(d),
        vor_vm_vm_vm(
            vand_vm_vm_vm(vreinterpret_vm_vd(d), vreinterpret_vm_vd(vcast_vd_d(-0.0))),
            vreinterpret_vm_vd(m),
        ),
    ))
}

#[inline(always)]
pub(crate) unsafe fn xatan2_u1(y: VDouble, x: VDouble) -> VDouble {
    let o = vlt_vo_vd_vd(vabs_vd_vd(x), vcast_vd_d(5.562_684_646_268_01e-309));
    let x = vsel_vd_vo_vd_vd(o, vmul_vd_vd_vd(x, vcast_vd_d((1u64 << 53) as f64)), x);
    let y = vsel_vd_vo_vd_vd(o, vmul_vd_vd_vd(y, vcast_vd_d((1u64 << 53) as f64)), y);

    let d = atan2k_u1(
        vcast_vd2_vd_vd(vabs_vd_vd(y), vcast_vd_d(0.0)),
        vcast_vd2_vd_vd(x, vcast_vd_d(0.0)),
    );
    let mut r = vadd_vd_vd_vd(vd2getx_vd_vd2(d), vd2gety_vd_vd2(d));

    r = vmulsign_vd_vd_vd(r, x);

    r = vsel_vd_vo_vd_vd(
        vor_vo_vo_vo(visinf_vo_vd(x), veq_vo_vd_vd(x, vcast_vd_d(0.0))),
        vsub_vd_vd_vd(
            vcast_vd_d(M_PI / 2.0),
            visinf2_vd_vd_vd(x, vmulsign_vd_vd_vd(vcast_vd_d(M_PI / 2.0), x)),
        ),
        r,
    );

    r = vsel_vd_vo_vd_vd(
        visinf_vo_vd(y),
        vsub_vd_vd_vd(
            vcast_vd_d(M_PI / 2.0),
            visinf2_vd_vd_vd(x, vmulsign_vd_vd_vd(vcast_vd_d(M_PI / 4.0), x)),
        ),
        r,
    );

    r = vsel_vd_vo_vd_vd(
        veq_vo_vd_vd(y, vcast_vd_d(0.0)),
        vreinterpret_vd_vm(vand_vm_vo64_vm(
            vsignbit_vo_vd(x),
            vreinterpret_vm_vd(vcast_vd_d(M_PI)),
        )),
        r,
    );

    r = vreinterpret_vd_vm(vor_vm_vo64_vm(
        vor_vo_vo_vo(visnan_vo_vd(x), visnan_vo_vd(y)),
        vreinterpret_vm_vd(vmulsign_vd_vd_vd(r, y)),
    ));

    r
}

#[inline(always)]
pub(crate) unsafe fn xasin_u1(d: VDouble) -> VDouble {
    let o = vlt_vo_vd_vd(vabs_vd_vd(d), vcast_vd_d(0.5));

    let x2 = vsel_vd_vo_vd_vd(
        o,
        vmul_vd_vd_vd(d, d),
        vmul_vd_vd_vd(
            vsub_vd_vd_vd(vcast_vd_d(1.0), vabs_vd_vd(d)),
            vcast_vd_d(0.5),
        ),
    );

    let mut u: VDouble;

    let mut x = vsel_vd2_vo_vd2_vd2(
        o,
        vcast_vd2_vd_vd(vabs_vd_vd(d), vcast_vd_d(0.0)),
        ddsqrt_vd2_vd(x2),
    );

    x = vsel_vd2_vo_vd2_vd2(
        veq_vo_vd_vd(vabs_vd_vd(d), vcast_vd_d(1.0)),
        vcast_vd2_d_d(0.0, 0.0),
        x,
    );

    let x4 = vmul_vd_vd_vd(x2, x2);
    let x8 = vmul_vd_vd_vd(x4, x4);
    let x16 = vmul_vd_vd_vd(x8, x8);

    u = poly12d(
        x2,
        x4,
        x8,
        x16,
        3.161_587_650_653_934_6e-2,
        -1.581_918_243_329_996_6e-2,
        1.929_045_477_267_910_7e-2,
        6.606_077_476_277_171e-3,
        1.215_360_525_577_377_3e-2,
        1.388_715_184_501_609_2e-2,
        1.735_956_991_223_614_6e-2,
        2.237_176_181_932_048_3e-2,
        3.038_195_928_038_132_2e-2,
        4.464_285_681_377_102_4e-2,
        7.500_000_000_378_582e-2,
        1.666_666_666_666_497_5e-1,
    );

    u = vmul_vd_vd_vd(u, vmul_vd_vd_vd(x2, vd2getx_vd_vd2(x)));

    let y = ddsub_vd2_vd2_vd(
        ddsub_vd2_vd2_vd2(
            vcast_vd2_d_d(
                3.141_592_653_589_793 / 4.0,
                1.224_646_799_147_353_2e-16 / 4.0,
            ),
            x,
        ),
        u,
    );

    let r = vsel_vd_vo_vd_vd(
        o,
        vadd_vd_vd_vd(u, vd2getx_vd_vd2(x)),
        vmul_vd_vd_vd(
            vadd_vd_vd_vd(vd2getx_vd_vd2(y), vd2gety_vd_vd2(y)),
            vcast_vd_d(2.0),
        ),
    );

    vmulsign_vd_vd_vd(r, d)
}

#[inline(always)]
pub(crate) unsafe fn xacos_u1(d: VDouble) -> VDouble {
    let o = vlt_vo_vd_vd(vabs_vd_vd(d), vcast_vd_d(0.5));

    let x2 = vsel_vd_vo_vd_vd(
        o,
        vmul_vd_vd_vd(d, d),
        vmul_vd_vd_vd(
            vsub_vd_vd_vd(vcast_vd_d(1.0), vabs_vd_vd(d)),
            vcast_vd_d(0.5),
        ),
    );
    let mut u: VDouble;

    let mut x = vsel_vd2_vo_vd2_vd2(
        o,
        vcast_vd2_vd_vd(vabs_vd_vd(d), vcast_vd_d(0.0)),
        ddsqrt_vd2_vd(x2),
    );

    x = vsel_vd2_vo_vd2_vd2(
        veq_vo_vd_vd(vabs_vd_vd(d), vcast_vd_d(1.0)),
        vcast_vd2_d_d(0.0, 0.0),
        x,
    );

    let x4 = vmul_vd_vd_vd(x2, x2);
    let x8 = vmul_vd_vd_vd(x4, x4);
    let x16 = vmul_vd_vd_vd(x8, x8);

    u = poly12d(
        x2,
        x4,
        x8,
        x16,
        3.161_587_650_653_934_6e-2,
        -1.581_918_243_329_996_6e-2,
        1.929_045_477_267_910_7e-2,
        6.606_077_476_277_171e-3,
        1.215_360_525_577_377_3e-2,
        1.388_715_184_501_609_2e-2,
        1.735_956_991_223_614_6e-2,
        2.237_176_181_932_048_3e-2,
        3.038_195_928_038_132_2e-2,
        4.464_285_681_377_102_4e-2,
        7.500_000_000_378_582e-2,
        1.666_666_666_666_497_5e-1,
    );

    u = vmul_vd_vd_vd(u, vmul_vd_vd_vd(x2, vd2getx_vd_vd2(x)));

    let mut y = ddsub_vd2_vd2_vd2(
        vcast_vd2_d_d(
            3.141_592_653_589_793 / 2.0,
            1.224_646_799_147_353_2e-16 / 2.0,
        ),
        ddadd_vd2_vd_vd(
            vmulsign_vd_vd_vd(vd2getx_vd_vd2(x), d),
            vmulsign_vd_vd_vd(u, d),
        ),
    );
    x = ddadd_vd2_vd2_vd(x, u);

    y = vsel_vd2_vo_vd2_vd2(o, y, ddscale_vd2_vd2_vd(x, vcast_vd_d(2.0)));

    y = vsel_vd2_vo_vd2_vd2(
        vandnot_vo_vo_vo(o, vlt_vo_vd_vd(d, vcast_vd_d(0.0))),
        ddsub_vd2_vd2_vd2(
            vcast_vd2_d_d(3.141_592_653_589_793, 1.224_646_799_147_353_2e-16),
            y,
        ),
        y,
    );

    vadd_vd_vd_vd(vd2getx_vd_vd2(y), vd2gety_vd_vd2(y))
}

#[inline(always)]
pub(crate) unsafe fn xatan_u1(d: VDouble) -> VDouble {
    let d2 = atan2k_u1(
        vcast_vd2_vd_vd(vabs_vd_vd(d), vcast_vd_d(0.0)),
        vcast_vd2_d_d(1.0, 0.0),
    );

    let mut r = vadd_vd_vd_vd(vd2getx_vd_vd2(d2), vd2gety_vd_vd2(d2));

    r = vsel_vd_vo_vd_vd(visinf_vo_vd(d), vcast_vd_d(1.570_796_326_794_896_6), r);

    vmulsign_vd_vd_vd(r, d)
}

#[inline(always)]
pub(crate) unsafe fn xlog_u1(d: VDouble) -> VDouble {
    #[cfg(not(target_feature = "avx512f"))]
    let (m, e) = {
        let o = vlt_vo_vd_vd(d, vcast_vd_d(SLEEF_DBL_MIN));
        let d = vsel_vd_vo_vd_vd(
            o,
            vmul_vd_vd_vd(d, vcast_vd_d((1u64 << 32) as f64 * (1u64 << 32) as f64)),
            d,
        );
        let mut e = vilogb2k_vi_vd(vmul_vd_vd_vd(d, vcast_vd_d(1.0 / 0.75)));
        let m = vldexp3_vd_vd_vi(d, vneg_vi_vi(e));
        e = vsel_vi_vo_vi_vi(vcast_vo32_vo64(o), vsub_vi_vi_vi(e, vcast_vi_i(64)), e);
        (m, e)
    };

    #[cfg(target_feature = "avx512f")]
    let (m, e) = {
        use crate::arch_simd::sleef::arch::helper_avx512::{vgetmant_vd_vd, vgetexp_vd_vd};
        
        let mut e = vgetexp_vd_vd(vmul_vd_vd_vd(d, vcast_vd_d(1.0 / 0.75)));
        e = vsel_vd_vo_vd_vd(vispinf_vo_vd(e), vcast_vd_d(1024.0), e);
        let m = vgetmant_vd_vd(d);
        (m, e)
    };

    let x: VDouble2 = dddiv_vd2_vd2_vd2(
        ddadd2_vd2_vd_vd(vcast_vd_d(-1.0), m),
        ddadd2_vd2_vd_vd(vcast_vd_d(1.0), m),
    );
    let x2: VDouble = vmul_vd_vd_vd(vd2getx_vd_vd2(x), vd2getx_vd_vd2(x));

    let x4 = vmul_vd_vd_vd(x2, x2);
    let x8 = vmul_vd_vd_vd(x4, x4);
    let t: VDouble = poly7d(
        x2,
        x4,
        x8,
        1.532_076_988_502_701_4e-1,
        1.525_629_051_003_428_7e-1,
        1.818_605_932_937_786e-1,
        2.222_214_519_839_38e-1,
        2.857_142_932_794_299_3e-1,
        3.999_999_999_635_252e-1,
        6.666_666_666_667_334e-1,
    );

    #[cfg(not(target_feature = "avx512f"))]
    let mut s = ddmul_vd2_vd2_vd(
        vcast_vd2_d_d(0.693_147_180_559_945_3, 2.319_046_813_846_299_6e-17),
        vcast_vd_vi(e),
    );

    #[cfg(target_feature = "avx512f")]
    let mut s = ddmul_vd2_vd2_vd(
        vcast_vd2_d_d(0.693147180559945286226764, 2.319046813846299558417771e-17),
        e,
    );

    s = ddadd_vd2_vd2_vd2(s, ddscale_vd2_vd2_vd(x, vcast_vd_d(2.0)));
    s = ddadd_vd2_vd2_vd(s, vmul_vd_vd_vd(vmul_vd_vd_vd(x2, vd2getx_vd_vd2(x)), t));

    let mut r = vadd_vd_vd_vd(vd2getx_vd_vd2(s), vd2gety_vd_vd2(s));

    #[cfg(not(target_feature = "avx512f"))]
    {
        r = vsel_vd_vo_vd_vd(vispinf_vo_vd(d), vcast_vd_d(f64::INFINITY), r);
        r = vsel_vd_vo_vd_vd(
            vor_vo_vo_vo(vlt_vo_vd_vd(d, vcast_vd_d(0.0)), visnan_vo_vd(d)),
            vcast_vd_d(f64::NAN),
            r,
        );
        r = vsel_vd_vo_vd_vd(
            veq_vo_vd_vd(d, vcast_vd_d(0.0)),
            vcast_vd_d(f64::NEG_INFINITY),
            r,
        );
    }

    #[cfg(target_feature = "avx512f")]
    {
        use crate::arch_simd::sleef::arch::helper_avx512::{vfixup_vd_vd_vd_vi2_i, vcast_vi2_i};
        r = vfixup_vd_vd_vd_vi2_i::<0>(
            r,
            d,
            vcast_vi2_i((4 << (2 * 4)) | (3 << (4 * 4)) | (5 << (5 * 4)) | (2 << (6 * 4))),
        );
    }

    r
}

#[inline(always)]
pub(crate) unsafe fn xcbrt_u1(d: VDouble) -> VDouble {
    let mut x: VDouble;
    let mut y: VDouble;
    let mut z: VDouble;

    let mut q2 = vcast_vd2_d_d(1.0, 0.0);
    let mut u: VDouble2;
    let mut v: VDouble2;

    #[cfg(target_feature = "avx512f")]
    let s = d;

    let e: VInt = vadd_vi_vi_vi(vilogbk_vi_vd(vabs_vd_vd(d)), vcast_vi_i(1));
    let mut d = vldexp2_vd_vd_vi(d, vneg_vi_vi(e));

    let t: VDouble = vadd_vd_vd_vd(vcast_vd_vi(e), vcast_vd_d(6144.0));
    let qu: VInt = vtruncate_vi_vd(vmul_vd_vd_vd(t, vcast_vd_d(1.0 / 3.0)));
    let re: VInt = vtruncate_vi_vd(vsub_vd_vd_vd(
        t,
        vmul_vd_vd_vd(vcast_vd_vi(qu), vcast_vd_d(3.0)),
    ));

    q2 = vsel_vd2_vo_vd2_vd2(
        vcast_vo64_vo32(veq_vo_vi_vi(re, vcast_vi_i(1))),
        vcast_vd2_d_d(1.259_921_049_894_873_2, -2.589_933_375_300_507e-17),
        q2,
    );
    q2 = vsel_vd2_vo_vd2_vd2(
        vcast_vo64_vo32(veq_vo_vi_vi(re, vcast_vi_i(2))),
        vcast_vd2_d_d(1.587_401_051_968_199_6, -1.086_900_819_419_782_3e-16),
        q2,
    );

    q2 = vd2setxy_vd2_vd_vd(
        vmulsign_vd_vd_vd(vd2getx_vd_vd2(q2), d),
        vmulsign_vd_vd_vd(vd2gety_vd_vd2(q2), d),
    );
    d = vabs_vd_vd(d);

    x = vcast_vd_d(-0.640_245_898_480_692_9);
    x = vmla_vd_vd_vd_vd(x, d, vcast_vd_d(2.961_551_030_200_395));
    x = vmla_vd_vd_vd_vd(x, d, vcast_vd_d(-5.733_530_609_229_478));
    x = vmla_vd_vd_vd_vd(x, d, vcast_vd_d(6.039_903_689_894_587_5));
    x = vmla_vd_vd_vd_vd(x, d, vcast_vd_d(-3.858_419_355_104_45));
    x = vmla_vd_vd_vd_vd(x, d, vcast_vd_d(2.230_727_530_249_661));

    y = vmul_vd_vd_vd(x, x);
    y = vmul_vd_vd_vd(y, y);
    x = vsub_vd_vd_vd(
        x,
        vmul_vd_vd_vd(vmlapn_vd_vd_vd_vd(d, y, x), vcast_vd_d(1.0 / 3.0)),
    );

    z = x;

    u = ddmul_vd2_vd_vd(x, x);
    u = ddmul_vd2_vd2_vd2(u, u);
    u = ddmul_vd2_vd2_vd(u, d);
    u = ddadd2_vd2_vd2_vd(u, vneg_vd_vd(x));
    y = vadd_vd_vd_vd(vd2getx_vd_vd2(u), vd2gety_vd_vd2(u));

    y = vmul_vd_vd_vd(vmul_vd_vd_vd(vcast_vd_d(-2.0 / 3.0), y), z);
    v = ddadd2_vd2_vd2_vd(ddmul_vd2_vd_vd(z, z), y);
    v = ddmul_vd2_vd2_vd(v, d);
    v = ddmul_vd2_vd2_vd2(v, q2);
    z = vldexp2_vd_vd_vi(
        vadd_vd_vd_vd(vd2getx_vd_vd2(v), vd2gety_vd_vd2(v)),
        vsub_vi_vi_vi(qu, vcast_vi_i(2048)),
    );

    #[cfg(not(target_feature = "avx512f"))]
    {
        z = vsel_vd_vo_vd_vd(
            visinf_vo_vd(d),
            vmulsign_vd_vd_vd(vcast_vd_d(f64::INFINITY), vd2getx_vd_vd2(q2)),
            z,
        );
        z = vsel_vd_vo_vd_vd(
            veq_vo_vd_vd(d, vcast_vd_d(0.0)),
            vreinterpret_vd_vm(vsignbit_vm_vd(vd2getx_vd_vd2(q2))),
            z,
        );
    }

    #[cfg(target_feature = "avx512f")]
    {
        z = vsel_vd_vo_vd_vd(
            visinf_vo_vd(s),
            vmulsign_vd_vd_vd(vcast_vd_d(f64::INFINITY), s),
            z,
        );
        z = vsel_vd_vo_vd_vd(
            veq_vo_vd_vd(s, vcast_vd_d(0.0)),
            vmulsign_vd_vd_vd(vcast_vd_d(0.0), s),
            z,
        );
    }

    z
}

#[inline(always)]
pub(crate) unsafe fn xexp(d: VDouble) -> VDouble {
    let u = vrint_vd_vd(vmul_vd_vd_vd(d, vcast_vd_d(R_LN2)));
    let q = vrint_vi_vd(u);

    let mut s = vmla_vd_vd_vd_vd(u, vcast_vd_d(-L2U), d);
    s = vmla_vd_vd_vd_vd(u, vcast_vd_d(-L2L), s);

    let mut u: VDouble;

    #[cfg(target_feature = "fma")]
    {
        use helper::vfma_vd_vd_vd_vd;
        let s2 = vmul_vd_vd_vd(s, s);
        let s4 = vmul_vd_vd_vd(s2, s2);
        let s8 = vmul_vd_vd_vd(s4, s4);

        u = poly10d(
            s,
            s2,
            s4,
            s8,
            0.2081276378237164457e-8,
            0.2511210703042288022e-7,
            0.2755762628169491192e-6,
            0.2755723402025388239e-5,
            0.2480158687479686264e-4,
            0.1984126989855865850e-3,
            0.1388888888914497797e-2,
            0.8333333333314938210e-2,
            0.4166666666666602598e-1,
            0.1666666666666669072e+0,
        );

        u = vfma_vd_vd_vd_vd(u, s, vcast_vd_d(0.5000000000000000000e+0));
        u = vfma_vd_vd_vd_vd(u, s, vcast_vd_d(0.1000000000000000000e+1));
        u = vfma_vd_vd_vd_vd(u, s, vcast_vd_d(0.1000000000000000000e+1));
    }

    #[cfg(not(target_feature = "fma"))]
    {
        let s2 = vmul_vd_vd_vd(s, s);
        let s4 = vmul_vd_vd_vd(s2, s2);
        let s8 = vmul_vd_vd_vd(s4, s4);

        u = poly10d(
            s,
            s2,
            s4,
            s8,
            2.088_606_211_072_837e-9,
            2.511_129_308_928_765_2e-8,
            2.755_739_112_349_004_7e-7,
            2.755_723_629_119_288_3e-6,
            2.480_158_715_923_547_3e-5,
            0.000_198_412_698_960_509_2,
            0.001_388_888_888_897_745,
            0.008_333_333_333_316_527,
            0.041_666_666_666_666_505,
            0.166_666_666_666_666_85,
        );

        u = vmla_vd_vd_vd_vd(u, s, vcast_vd_d(5e-1));
        u = vadd_vd_vd_vd(vcast_vd_d(1.0), vmla_vd_vd_vd_vd(vmul_vd_vd_vd(s, s), u, s));
    }

    u = vldexp2_vd_vd_vi(u, q);

    let o = vgt_vo_vd_vd(d, vcast_vd_d(LOG_DBL_MAX));
    u = vsel_vd_vo_vd_vd(o, vcast_vd_d(f64::INFINITY), u);
    u = vreinterpret_vd_vm(vandnot_vm_vo64_vm(
        vlt_vo_vd_vd(d, vcast_vd_d(-1000.0)),
        vreinterpret_vm_vd(u),
    ));

    u
}

#[inline(always)]
unsafe fn logk(d: VDouble) -> VDouble2 {
    let mut x: VDouble2;

    let mut s: VDouble2;

    #[cfg(not(target_feature = "avx512f"))]
    let (m, e) = {
        let o = vlt_vo_vd_vd(d, vcast_vd_d(SLEEF_DBL_MIN));
        let d = vsel_vd_vo_vd_vd(
            o,
            vmul_vd_vd_vd(d, vcast_vd_d((1u64 << 32) as f64 * (1u64 << 32) as f64)),
            d,
        );
        let mut e = vilogb2k_vi_vd(vmul_vd_vd_vd(d, vcast_vd_d(1.0 / 0.75)));
        let m = vldexp3_vd_vd_vi(d, vneg_vi_vi(e));
        e = vsel_vi_vo_vi_vi(vcast_vo32_vo64(o), vsub_vi_vi_vi(e, vcast_vi_i(64)), e);
        (m, e)
    };

    #[cfg(target_feature = "avx512f")]
    let (m, e) = {
        use crate::arch_simd::sleef::arch::helper_avx512::{vgetmant_vd_vd, vgetexp_vd_vd};
        let mut e = vgetexp_vd_vd(vmul_vd_vd_vd(d, vcast_vd_d(1.0 / 0.75)));
        e = vsel_vd_vo_vd_vd(vispinf_vo_vd(e), vcast_vd_d(1024.0), e);
        let m = vgetmant_vd_vd(d);
        (m, e)
    };

    x = dddiv_vd2_vd2_vd2(
        ddadd2_vd2_vd_vd(vcast_vd_d(-1.0), m),
        ddadd2_vd2_vd_vd(vcast_vd_d(1.0), m),
    );
    let x2: VDouble2 = ddsqu_vd2_vd2(x);

    let x4 = vmul_vd_vd_vd(vd2getx_vd_vd2(x2), vd2getx_vd_vd2(x2));
    let x8 = vmul_vd_vd_vd(x4, x4);
    let x16 = vmul_vd_vd_vd(x8, x8);
    let t: VDouble = poly9d(
        vd2getx_vd_vd2(x2),
        x4,
        x8,
        x16,
        0.116_255_524_079_935_04,
        0.103_239_680_901_072_95,
        0.117_754_809_412_464,
        0.133_329_810_868_462_74,
        0.153_846_227_114_512_26,
        0.181_818_180_850_050_78,
        0.222_222_222_230_083_56,
        0.285_714_285_714_249_17,
        0.400_000_000_000_000_1,
    );

    let c = vcast_vd2_d_d(0.666_666_666_666_666_6, 3.805_549_625_424_120_6e-17);

    #[cfg(not(target_feature = "avx512f"))]
    {
        s = ddmul_vd2_vd2_vd(
            vcast_vd2_d_d(0.693_147_180_559_945_3, 2.319_046_813_846_299_6e-17),
            vcast_vd_vi(e),
        );
    }

    #[cfg(target_feature = "avx512f")]
    {
        s = ddmul_vd2_vd2_vd(
            vcast_vd2_d_d(0.693147180559945286226764, 2.319046813846299558417771e-17),
            e,
        );
    }

    s = ddadd_vd2_vd2_vd2(s, ddscale_vd2_vd2_vd(x, vcast_vd_d(2.0)));
    x = ddmul_vd2_vd2_vd2(x2, x);
    s = ddadd_vd2_vd2_vd2(s, ddmul_vd2_vd2_vd2(x, c));
    x = ddmul_vd2_vd2_vd2(x2, x);
    s = ddadd_vd2_vd2_vd2(s, ddmul_vd2_vd2_vd(x, t));

    s
}

#[inline(always)]
unsafe fn expk(d: VDouble2) -> VDouble {
    let u = vmul_vd_vd_vd(
        vadd_vd_vd_vd(vd2getx_vd_vd2(d), vd2gety_vd_vd2(d)),
        vcast_vd_d(R_LN2),
    );
    let dq = vrint_vd_vd(u);
    let q = vrint_vi_vd(dq);
    let mut s: VDouble2;
    let mut t: VDouble2;

    s = ddadd2_vd2_vd2_vd(d, vmul_vd_vd_vd(dq, vcast_vd_d(-L2U)));
    s = ddadd2_vd2_vd2_vd(s, vmul_vd_vd_vd(dq, vcast_vd_d(-L2L)));

    s = ddnormalize_vd2_vd2(s);

    let s2 = vmul_vd_vd_vd(vd2getx_vd_vd2(s), vd2getx_vd_vd2(s));
    let s4 = vmul_vd_vd_vd(s2, s2);
    let s8 = vmul_vd_vd_vd(s4, s4);

    let mut u = poly10d(
        vd2getx_vd_vd2(s),
        s2,
        s4,
        s8,
        2.510_696_834_209_504_2e-8,
        2.762_861_667_702_706_5e-7,
        2.755_724_967_250_235_7e-6,
        2.480_149_739_898_198e-5,
        0.000_198_412_698_809_069_8,
        0.001_388_888_893_997_713,
        0.008_333_333_333_323_714,
        0.041_666_666_666_540_95,
        0.166_666_666_666_666_74,
        0.500_000_000_000_001,
    );

    t = ddadd_vd2_vd_vd2(vcast_vd_d(1.0), s);
    t = ddadd_vd2_vd2_vd2(t, ddmul_vd2_vd2_vd(ddsqu_vd2_vd2(s), u));

    u = vadd_vd_vd_vd(vd2getx_vd_vd2(t), vd2gety_vd_vd2(t));

    u = vldexp2_vd_vd_vi(u, q);

    u = vreinterpret_vd_vm(vandnot_vm_vo64_vm(
        vlt_vo_vd_vd(vd2getx_vd_vd2(d), vcast_vd_d(-1000.0)),
        vreinterpret_vm_vd(u),
    ));

    u
}

#[inline(always)]
pub(crate) unsafe fn xpow(x: VDouble, y: VDouble) -> VDouble {
    let yisint = visint_vo_vd(y);
    let yisodd = vand_vo_vo_vo(visodd_vo_vd(y), yisint);

    let d = ddmul_vd2_vd2_vd(logk(vabs_vd_vd(x)), y);
    let mut result = expk(d);

    let o = vgt_vo_vd_vd(vd2getx_vd_vd2(d), vcast_vd_d(LOG_DBL_MAX));
    result = vsel_vd_vo_vd_vd(o, vcast_vd_d(f64::INFINITY), result);

    result = vmul_vd_vd_vd(
        result,
        vsel_vd_vo_vd_vd(
            vgt_vo_vd_vd(x, vcast_vd_d(0.0)),
            vcast_vd_d(1.0),
            vsel_vd_vo_vd_vd(
                yisint,
                vsel_vd_vo_vd_vd(yisodd, vcast_vd_d(-1.0), vcast_vd_d(1.0)),
                vcast_vd_d(f64::NAN),
            ),
        ),
    );

    let efx = vmulsign_vd_vd_vd(vsub_vd_vd_vd(vabs_vd_vd(x), vcast_vd_d(1.0)), y);
    result = vsel_vd_vo_vd_vd(
        visinf_vo_vd(y),
        vreinterpret_vd_vm(vandnot_vm_vo64_vm(
            vlt_vo_vd_vd(efx, vcast_vd_d(0.0)),
            vreinterpret_vm_vd(vsel_vd_vo_vd_vd(
                veq_vo_vd_vd(efx, vcast_vd_d(0.0)),
                vcast_vd_d(1.0),
                vcast_vd_d(f64::INFINITY),
            )),
        )),
        result,
    );

    result = vsel_vd_vo_vd_vd(
        vor_vo_vo_vo(visinf_vo_vd(x), veq_vo_vd_vd(x, vcast_vd_d(0.0))),
        vmulsign_vd_vd_vd(
            vsel_vd_vo_vd_vd(
                vxor_vo_vo_vo(vsignbit_vo_vd(y), veq_vo_vd_vd(x, vcast_vd_d(0.0))),
                vcast_vd_d(0.0),
                vcast_vd_d(f64::INFINITY),
            ),
            vsel_vd_vo_vd_vd(yisodd, x, vcast_vd_d(1.0)),
        ),
        result,
    );

    result = vreinterpret_vd_vm(vor_vm_vo64_vm(
        vor_vo_vo_vo(visnan_vo_vd(x), visnan_vo_vd(y)),
        vreinterpret_vm_vd(result),
    ));

    result = vsel_vd_vo_vd_vd(
        vor_vo_vo_vo(
            veq_vo_vd_vd(y, vcast_vd_d(0.0)),
            veq_vo_vd_vd(x, vcast_vd_d(1.0)),
        ),
        vcast_vd_d(1.0),
        result,
    );

    result
}

#[inline(always)]
unsafe fn expk2(d: VDouble2) -> VDouble2 {
    let u = vmul_vd_vd_vd(
        vadd_vd_vd_vd(vd2getx_vd_vd2(d), vd2gety_vd_vd2(d)),
        vcast_vd_d(R_LN2),
    );
    let dq = vrint_vd_vd(u);
    let q = vrint_vi_vd(dq);
    let mut s: VDouble2;
    let mut t: VDouble2;

    s = ddadd2_vd2_vd2_vd(d, vmul_vd_vd_vd(dq, vcast_vd_d(-L2U)));
    s = ddadd2_vd2_vd2_vd(s, vmul_vd_vd_vd(dq, vcast_vd_d(-L2L)));

    let s2 = ddsqu_vd2_vd2(s);
    let s4 = ddsqu_vd2_vd2(s2);
    let s8 = vmul_vd_vd_vd(vd2getx_vd_vd2(s4), vd2getx_vd_vd2(s4));

    let u = poly10d(
        vd2getx_vd_vd2(s),
        vd2getx_vd_vd2(s2),
        vd2getx_vd_vd2(s4),
        s8,
        1.602_472_219_709_932e-10,
        2.092_255_183_563_157e-9,
        2.505_230_023_782_644_5e-8,
        2.755_724_800_902_135_3e-7,
        2.755_731_892_386_044_4e-6,
        2.480_158_735_605_815e-5,
        1.984_126_984_148_071_9e-4,
        1.388_888_888_886_763_3e-3,
        8.333_333_333_333_347e-3,
        4.166_666_666_666_67e-2,
    );

    t = ddadd_vd2_vd_vd2(
        vcast_vd_d(0.5),
        ddmul_vd2_vd2_vd(s, vcast_vd_d(1.666_666_666_666_666_6e-1)),
    );
    t = ddadd_vd2_vd_vd2(vcast_vd_d(1.0), ddmul_vd2_vd2_vd2(t, s));
    t = ddadd_vd2_vd_vd2(vcast_vd_d(1.0), ddmul_vd2_vd2_vd2(t, s));
    t = ddadd_vd2_vd2_vd2(t, ddmul_vd2_vd2_vd(s4, u));

    t = vd2setx_vd2_vd2_vd(t, vldexp2_vd_vd_vi(vd2getx_vd_vd2(t), q));
    t = vd2sety_vd2_vd2_vd(t, vldexp2_vd_vd_vi(vd2gety_vd_vd2(t), q));

    t = vd2setx_vd2_vd2_vd(
        t,
        vreinterpret_vd_vm(vandnot_vm_vo64_vm(
            vlt_vo_vd_vd(vd2getx_vd_vd2(d), vcast_vd_d(-1000.0)),
            vreinterpret_vm_vd(vd2getx_vd_vd2(t)),
        )),
    );
    t = vd2sety_vd2_vd2_vd(
        t,
        vreinterpret_vd_vm(vandnot_vm_vo64_vm(
            vlt_vo_vd_vd(vd2getx_vd_vd2(d), vcast_vd_d(-1000.0)),
            vreinterpret_vm_vd(vd2gety_vd_vd2(t)),
        )),
    );

    t
}

#[inline(always)]
pub(crate) unsafe fn xsinh(x: VDouble) -> VDouble {
    let mut y = vabs_vd_vd(x);

    let mut d = expk2(vcast_vd2_vd_vd(y, vcast_vd_d(0.0)));

    d = ddsub_vd2_vd2_vd2(d, ddrec_vd2_vd2(d));
    y = vmul_vd_vd_vd(
        vadd_vd_vd_vd(vd2getx_vd_vd2(d), vd2gety_vd_vd2(d)),
        vcast_vd_d(0.5),
    );

    y = vsel_vd_vo_vd_vd(
        vor_vo_vo_vo(
            vgt_vo_vd_vd(vabs_vd_vd(x), vcast_vd_d(710.0)),
            visnan_vo_vd(y),
        ),
        vcast_vd_d(f64::INFINITY),
        y,
    );

    y = vmulsign_vd_vd_vd(y, x);

    y = vreinterpret_vd_vm(vor_vm_vo64_vm(visnan_vo_vd(x), vreinterpret_vm_vd(y)));

    y
}

#[inline(always)]
pub(crate) unsafe fn xcosh(x: VDouble) -> VDouble {
    let mut y = vabs_vd_vd(x);

    let mut d = expk2(vcast_vd2_vd_vd(y, vcast_vd_d(0.0)));

    d = ddadd_vd2_vd2_vd2(d, ddrec_vd2_vd2(d));
    y = vmul_vd_vd_vd(
        vadd_vd_vd_vd(vd2getx_vd_vd2(d), vd2gety_vd_vd2(d)),
        vcast_vd_d(0.5),
    );

    y = vsel_vd_vo_vd_vd(
        vor_vo_vo_vo(
            vgt_vo_vd_vd(vabs_vd_vd(x), vcast_vd_d(710.0)),
            visnan_vo_vd(y),
        ),
        vcast_vd_d(f64::INFINITY),
        y,
    );

    y = vreinterpret_vd_vm(vor_vm_vo64_vm(visnan_vo_vd(x), vreinterpret_vm_vd(y)));

    y
}

#[inline(always)]
pub(crate) unsafe fn xtanh(x: VDouble) -> VDouble {
    let mut y = vabs_vd_vd(x);

    let d = expk2(vcast_vd2_vd_vd(y, vcast_vd_d(0.0)));

    let e = ddrec_vd2_vd2(d);

    let d = dddiv_vd2_vd2_vd2(
        ddadd2_vd2_vd2_vd2(d, ddneg_vd2_vd2(e)),
        ddadd2_vd2_vd2_vd2(d, e),
    );
    y = vadd_vd_vd_vd(vd2getx_vd_vd2(d), vd2gety_vd_vd2(d));

    y = vsel_vd_vo_vd_vd(
        vor_vo_vo_vo(
            vgt_vo_vd_vd(vabs_vd_vd(x), vcast_vd_d(18.714973875)),
            visnan_vo_vd(y),
        ),
        vcast_vd_d(1.0),
        y,
    );

    y = vmulsign_vd_vd_vd(y, x);

    y = vreinterpret_vd_vm(vor_vm_vo64_vm(visnan_vo_vd(x), vreinterpret_vm_vd(y)));

    y
}

#[inline(always)]
unsafe fn logk2(d: VDouble2) -> VDouble2 {
    let mut s: VDouble2;
    let mut t: VDouble;

    let e = vilogbk_vi_vd(vmul_vd_vd_vd(vd2getx_vd_vd2(d), vcast_vd_d(1.0 / 0.75)));

    let m: VDouble2 = vd2setxy_vd2_vd_vd(
        vldexp2_vd_vd_vi(vd2getx_vd_vd2(d), vneg_vi_vi(e)),
        vldexp2_vd_vd_vi(vd2gety_vd_vd2(d), vneg_vi_vi(e)),
    );

    let x: VDouble2 = dddiv_vd2_vd2_vd2(
        ddadd2_vd2_vd2_vd(m, vcast_vd_d(-1.0)),
        ddadd2_vd2_vd2_vd(m, vcast_vd_d(1.0)),
    );
    let x2: VDouble2 = ddsqu_vd2_vd2(x);

    let x4 = vmul_vd_vd_vd(vd2getx_vd_vd2(x2), vd2getx_vd_vd2(x2));
    let x8 = vmul_vd_vd_vd(x4, x4);
    t = poly7d(
        vd2getx_vd_vd2(x2),
        x4,
        x8,
        0.138_604_363_904_671_68,
        0.131_699_838_841_615_37,
        0.153_914_168_346_271_95,
        0.181_816_523_941_564_6,
        0.222_222_246_326_620_35,
        0.285_714_285_511_134_1,
        0.400_000_000_000_914,
    );
    t = vmla_vd_vd_vd_vd(t, vd2getx_vd_vd2(x2), vcast_vd_d(0.666_666_666_666_664_9));

    s = ddmul_vd2_vd2_vd(
        vcast_vd2_d_d(0.693_147_180_559_945_3, 2.319_046_813_846_299_6e-17),
        vcast_vd_vi(e),
    );
    s = ddadd_vd2_vd2_vd2(s, ddscale_vd2_vd2_vd(x, vcast_vd_d(2.0)));
    s = ddadd_vd2_vd2_vd2(s, ddmul_vd2_vd2_vd(ddmul_vd2_vd2_vd2(x2, x), t));

    s
}

#[inline(always)]
pub(crate) unsafe fn xasinh(x: VDouble) -> VDouble {
    let mut y = vabs_vd_vd(x);

    let o = vgt_vo_vd_vd(y, vcast_vd_d(1.0));
    let mut d: VDouble2;

    d = vsel_vd2_vo_vd2_vd2(o, ddrec_vd2_vd(x), vcast_vd2_vd_vd(y, vcast_vd_d(0.0)));

    d = ddsqrt_vd2_vd2(ddadd2_vd2_vd2_vd(ddsqu_vd2_vd2(d), vcast_vd_d(1.0)));

    d = vsel_vd2_vo_vd2_vd2(o, ddmul_vd2_vd2_vd(d, y), d);

    d = logk2(ddnormalize_vd2_vd2(ddadd2_vd2_vd2_vd(d, x)));
    y = vadd_vd_vd_vd(vd2getx_vd_vd2(d), vd2gety_vd_vd2(d));

    y = vsel_vd_vo_vd_vd(
        vor_vo_vo_vo(
            vgt_vo_vd_vd(vabs_vd_vd(x), vcast_vd_d(SQRT_DBL_MAX)),
            visnan_vo_vd(y),
        ),
        vmulsign_vd_vd_vd(vcast_vd_d(f64::INFINITY), x),
        y,
    );

    y = vreinterpret_vd_vm(vor_vm_vo64_vm(visnan_vo_vd(x), vreinterpret_vm_vd(y)));

    y = vsel_vd_vo_vd_vd(visnegzero_vo_vd(x), vcast_vd_d(-0.0), y);

    y
}

#[inline(always)]
pub(crate) unsafe fn xacosh(x: VDouble) -> VDouble {
    let d = logk2(ddadd2_vd2_vd2_vd(
        ddmul_vd2_vd2_vd2(
            ddsqrt_vd2_vd2(ddadd2_vd2_vd_vd(x, vcast_vd_d(1.0))),
            ddsqrt_vd2_vd2(ddadd2_vd2_vd_vd(x, vcast_vd_d(-1.0))),
        ),
        x,
    ));
    let mut y = vadd_vd_vd_vd(vd2getx_vd_vd2(d), vd2gety_vd_vd2(d));

    y = vsel_vd_vo_vd_vd(
        vor_vo_vo_vo(
            vgt_vo_vd_vd(vabs_vd_vd(x), vcast_vd_d(SQRT_DBL_MAX)),
            visnan_vo_vd(y),
        ),
        vcast_vd_d(f64::INFINITY),
        y,
    );

    y = vreinterpret_vd_vm(vandnot_vm_vo64_vm(
        veq_vo_vd_vd(x, vcast_vd_d(1.0)),
        vreinterpret_vm_vd(y),
    ));

    y = vreinterpret_vd_vm(vor_vm_vo64_vm(
        vlt_vo_vd_vd(x, vcast_vd_d(1.0)),
        vreinterpret_vm_vd(y),
    ));

    y = vreinterpret_vd_vm(vor_vm_vo64_vm(visnan_vo_vd(x), vreinterpret_vm_vd(y)));

    y
}

#[inline(always)]
pub(crate) unsafe fn xatanh(x: VDouble) -> VDouble {
    let y = vabs_vd_vd(x);

    let d = logk2(dddiv_vd2_vd2_vd2(
        ddadd2_vd2_vd_vd(vcast_vd_d(1.0), y),
        ddadd2_vd2_vd_vd(vcast_vd_d(1.0), vneg_vd_vd(y)),
    ));

    let mut y = vreinterpret_vd_vm(vor_vm_vo64_vm(
        vgt_vo_vd_vd(y, vcast_vd_d(1.0)),
        vreinterpret_vm_vd(vsel_vd_vo_vd_vd(
            veq_vo_vd_vd(y, vcast_vd_d(1.0)),
            vcast_vd_d(f64::INFINITY),
            vmul_vd_vd_vd(
                vadd_vd_vd_vd(vd2getx_vd_vd2(d), vd2gety_vd_vd2(d)),
                vcast_vd_d(0.5),
            ),
        )),
    ));

    y = vmulsign_vd_vd_vd(y, x);

    y = vreinterpret_vd_vm(vor_vm_vo64_vm(
        vor_vo_vo_vo(visinf_vo_vd(x), visnan_vo_vd(y)),
        vreinterpret_vm_vd(y),
    ));

    y = vreinterpret_vd_vm(vor_vm_vo64_vm(visnan_vo_vd(x), vreinterpret_vm_vd(y)));

    y
}

#[inline(always)]
pub(crate) unsafe fn xexp2(d: VDouble) -> VDouble {
    let u = vrint_vd_vd(d);
    let s = vsub_vd_vd_vd(d, u);
    let q = vrint_vi_vd(u);

    let s2 = vmul_vd_vd_vd(s, s);
    let s4 = vmul_vd_vd_vd(s2, s2);
    let s8 = vmul_vd_vd_vd(s4, s4);

    let mut u = poly10d(
        s,
        s2,
        s4,
        s8,
        4.434_359_082_926_529_5e-10,
        7.073_164_598_085_707_4e-9,
        1.017_819_260_921_760_5e-7,
        1.321_543_872_511_327_6e-6,
        1.525_273_353_517_584_7e-5,
        1.540_353_045_101_147_8e-4,
        1.333_355_814_670_499e-3,
        9.618_129_107_597_6e-3,
        5.550_410_866_482_046_6e-2,
        2.402_265_069_591_012_2e-1,
    );

    u = vmla_vd_vd_vd_vd(u, s, vcast_vd_d(6.931_471_805_599_453e-1));

    #[cfg(target_feature = "fma")]
    {
        use helper::vfma_vd_vd_vd_vd;
        u = vfma_vd_vd_vd_vd(u, s, vcast_vd_d(1.0));
    }

    #[cfg(not(target_feature = "fma"))]
    {
        u = vd2getx_vd_vd2(ddnormalize_vd2_vd2(ddadd_vd2_vd_vd2(
            vcast_vd_d(1.0),
            ddmul_vd2_vd_vd(u, s),
        )));
    }

    u = vldexp2_vd_vd_vi(u, q);

    u = vsel_vd_vo_vd_vd(
        vge_vo_vd_vd(d, vcast_vd_d(1024.0)),
        vcast_vd_d(f64::INFINITY),
        u,
    );
    u = vreinterpret_vd_vm(vandnot_vm_vo64_vm(
        vlt_vo_vd_vd(d, vcast_vd_d(-2000.0)),
        vreinterpret_vm_vd(u),
    ));

    u
}

#[inline(always)]
pub(crate) unsafe fn xexp10(d: VDouble) -> VDouble {
    let u = vrint_vd_vd(vmul_vd_vd_vd(d, vcast_vd_d(LOG10_2)));
    let q = vrint_vi_vd(u);

    let mut s = vmla_vd_vd_vd_vd(u, vcast_vd_d(-L10_U), d);
    s = vmla_vd_vd_vd_vd(u, vcast_vd_d(-L10_L), s);

    let mut u = vcast_vd_d(2.411_463_498_334_267_7e-4);
    u = vmla_vd_vd_vd_vd(u, s, vcast_vd_d(1.157_488_415_217_187_4e-3));
    u = vmla_vd_vd_vd_vd(u, s, vcast_vd_d(5.013_975_546_789_734e-3));
    u = vmla_vd_vd_vd_vd(u, s, vcast_vd_d(1.959_762_320_720_533e-2));
    u = vmla_vd_vd_vd_vd(u, s, vcast_vd_d(6.808_936_399_446_784e-2));
    u = vmla_vd_vd_vd_vd(u, s, vcast_vd_d(2.069_958_494_722_676_2e-1));
    u = vmla_vd_vd_vd_vd(u, s, vcast_vd_d(5.393_829_292_058_536e-1));
    u = vmla_vd_vd_vd_vd(u, s, vcast_vd_d(1.171_255_148_908_541_7));
    u = vmla_vd_vd_vd_vd(u, s, vcast_vd_d(2.034_678_592_293_433));
    u = vmla_vd_vd_vd_vd(u, s, vcast_vd_d(2.650_949_055_239_206));
    u = vmla_vd_vd_vd_vd(u, s, vcast_vd_d(2.302_585_092_994_046));

    #[cfg(target_feature = "fma")]
    {
        use helper::vfma_vd_vd_vd_vd;
        u = vfma_vd_vd_vd_vd(u, s, vcast_vd_d(1.0));
    }
    #[cfg(not(target_feature = "fma"))]
    {
        u = vd2getx_vd_vd2(ddnormalize_vd2_vd2(ddadd_vd2_vd_vd2(
            vcast_vd_d(1.0),
            ddmul_vd2_vd_vd(u, s),
        )));
    }

    u = vldexp2_vd_vd_vi(u, q);

    u = vsel_vd_vo_vd_vd(
        vgt_vo_vd_vd(d, vcast_vd_d(308.254_715_559_916_7)),
        vcast_vd_d(f64::INFINITY),
        u,
    );
    u = vreinterpret_vd_vm(vandnot_vm_vo64_vm(
        vlt_vo_vd_vd(d, vcast_vd_d(-350.0)),
        vreinterpret_vm_vd(u),
    ));

    u
}

#[inline(always)]
pub(crate) unsafe fn xexpm1(a: VDouble) -> VDouble {
    let d = ddadd2_vd2_vd2_vd(expk2(vcast_vd2_vd_vd(a, vcast_vd_d(0.0))), vcast_vd_d(-1.0));
    let mut x = vadd_vd_vd_vd(vd2getx_vd_vd2(d), vd2gety_vd_vd2(d));

    x = vsel_vd_vo_vd_vd(
        vgt_vo_vd_vd(a, vcast_vd_d(709.782_712_893_384)),
        vcast_vd_d(f64::INFINITY),
        x,
    );

    x = vsel_vd_vo_vd_vd(
        vlt_vo_vd_vd(a, vcast_vd_d(-36.736_800_569_677_1)),
        vcast_vd_d(-1.0),
        x,
    );

    x = vsel_vd_vo_vd_vd(visnegzero_vo_vd(a), vcast_vd_d(-0.0), x);

    x
}

#[inline(always)]
pub(crate) unsafe fn xlog10(d: VDouble) -> VDouble {
    #[cfg(not(target_feature = "avx512f"))]
    let (m, e) = {
        let o = vlt_vo_vd_vd(d, vcast_vd_d(SLEEF_DBL_MIN));
        let d = vsel_vd_vo_vd_vd(o, vmul_vd_vd_vd(d, vcast_vd_d(2_f64.powi(64))), d);
        let mut e = vilogb2k_vi_vd(vmul_vd_vd_vd(d, vcast_vd_d(1.0 / 0.75)));
        let m = vldexp3_vd_vd_vi(d, vneg_vi_vi(e));
        e = vsel_vi_vo_vi_vi(vcast_vo32_vo64(o), vsub_vi_vi_vi(e, vcast_vi_i(64)), e);
        (m, e)
    };

    #[cfg(target_feature = "avx512f")]
    let (m, e) = {
        use crate::arch_simd::sleef::arch::helper_avx512::{vgetmant_vd_vd, vgetexp_vd_vd};
        let mut e = vgetexp_vd_vd(vmul_vd_vd_vd(d, vcast_vd_d(1.0 / 0.75)));
        e = vsel_vd_vo_vd_vd(vispinf_vo_vd(e), vcast_vd_d(1024.0), e);
        let m = vgetmant_vd_vd(d);
        (m, e)
    };

    let x: VDouble2 = dddiv_vd2_vd2_vd2(
        ddadd2_vd2_vd_vd(vcast_vd_d(-1.0), m),
        ddadd2_vd2_vd_vd(vcast_vd_d(1.0), m),
    );
    let x2: VDouble = vmul_vd_vd_vd(vd2getx_vd_vd2(x), vd2getx_vd_vd2(x));

    let x4 = vmul_vd_vd_vd(x2, x2);
    let x8 = vmul_vd_vd_vd(x4, x4);
    let t: VDouble = poly7d(
        x2,
        x4,
        x8,
        6.653_725_819_576_758e-2,
        6.625_722_782_820_834e-2,
        7.898_105_214_313_944e-2,
        9.650_955_035_715_275e-2,
        1.240_841_409_721_445e-1,
        1.737_177_927_454_605e-1,
        2.895_296_546_021_972_6e-1,
    );

    #[cfg(not(target_feature = "avx512f"))]
    let mut s = ddmul_vd2_vd2_vd(
        vcast_vd2_d_d(0.301_029_995_663_981_2, -2.803_728_127_785_170_4e-18),
        vcast_vd_vi(e),
    );

    #[cfg(target_feature = "avx512f")]
    let mut s = ddmul_vd2_vd2_vd(
        vcast_vd2_d_d(0.30102999566398119802, -2.803728127785170339e-18),
        e,
    );

    s = ddadd_vd2_vd2_vd2(
        s,
        ddmul_vd2_vd2_vd2(
            x,
            vcast_vd2_d_d(0.868_588_963_806_503_6, 1.143_005_969_409_638_9e-17),
        ),
    );
    s = ddadd_vd2_vd2_vd(s, vmul_vd_vd_vd(vmul_vd_vd_vd(x2, vd2getx_vd_vd2(x)), t));

    let mut r = vadd_vd_vd_vd(vd2getx_vd_vd2(s), vd2gety_vd_vd2(s));

    #[cfg(not(target_feature = "avx512f"))]
    {
        r = vsel_vd_vo_vd_vd(vispinf_vo_vd(d), vcast_vd_d(f64::INFINITY), r);
        r = vsel_vd_vo_vd_vd(
            vor_vo_vo_vo(vlt_vo_vd_vd(d, vcast_vd_d(0.0)), visnan_vo_vd(d)),
            vcast_vd_d(f64::NAN),
            r,
        );
        r = vsel_vd_vo_vd_vd(
            veq_vo_vd_vd(d, vcast_vd_d(0.0)),
            vcast_vd_d(f64::NEG_INFINITY),
            r,
        );
    }

    #[cfg(target_feature = "avx512f")]
    {
        use crate::arch_simd::sleef::arch::helper_avx512::{vfixup_vd_vd_vd_vi2_i, vcast_vi2_i};
        r = vfixup_vd_vd_vd_vi2_i::<0>(
            r,
            d,
            vcast_vi2_i((4 << (2 * 4)) | (3 << (4 * 4)) | (5 << (5 * 4)) | (2 << (6 * 4))),
        );
    }

    r
}

#[inline(always)]
pub(crate) unsafe fn xlog2(d: VDouble) -> VDouble {
    #[cfg(not(target_feature = "avx512f"))]
    let (m, e) = {
        let o = vlt_vo_vd_vd(d, vcast_vd_d(SLEEF_DBL_MIN));
        let d = vsel_vd_vo_vd_vd(o, vmul_vd_vd_vd(d, vcast_vd_d(2_f64.powi(64))), d);
        let mut e = vilogb2k_vi_vd(vmul_vd_vd_vd(d, vcast_vd_d(1.0 / 0.75)));
        let m = vldexp3_vd_vd_vi(d, vneg_vi_vi(e));
        e = vsel_vi_vo_vi_vi(vcast_vo32_vo64(o), vsub_vi_vi_vi(e, vcast_vi_i(64)), e);
        (m, e)
    };

    #[cfg(target_feature = "avx512f")]
    let (m, e) = {
        use crate::arch_simd::sleef::arch::helper_avx512::{vgetmant_vd_vd, vgetexp_vd_vd};
        let mut e = vgetexp_vd_vd(vmul_vd_vd_vd(d, vcast_vd_d(1.0 / 0.75)));
        e = vsel_vd_vo_vd_vd(vispinf_vo_vd(e), vcast_vd_d(1024.0), e);
        let m = vgetmant_vd_vd(d);
        (m, e)
    };

    let x: VDouble2 = dddiv_vd2_vd2_vd2(
        ddadd2_vd2_vd_vd(vcast_vd_d(-1.0), m),
        ddadd2_vd2_vd_vd(vcast_vd_d(1.0), m),
    );
    let x2: VDouble = vmul_vd_vd_vd(vd2getx_vd_vd2(x), vd2getx_vd_vd2(x));

    let x4 = vmul_vd_vd_vd(x2, x2);
    let x8 = vmul_vd_vd_vd(x4, x4);
    let t: VDouble = poly7d(
        x2,
        x4,
        x8,
        2.211_941_750_456_081_5e-1,
        2.200_768_693_152_277_7e-1,
        2.623_708_057_488_514_7e-1,
        3.205_977_477_944_495_5e-1,
        4.121_985_945_485_324_7e-1,
        5.770_780_162_997_059e-1,
        0.961_796_693_926_080_9,
    );

    #[cfg(not(target_feature = "avx512f"))]
    let mut s = ddadd2_vd2_vd_vd2(
        vcast_vd_vi(e),
        ddmul_vd2_vd2_vd2(
            x,
            vcast_vd2_d_d(2.885_390_081_777_926_8, 6.056_160_499_551_674e-18),
        ),
    );

    #[cfg(target_feature = "avx512f")]
    let mut s = ddadd2_vd2_vd_vd2(
        e,
        ddmul_vd2_vd2_vd2(
            x,
            vcast_vd2_d_d(2.885390081777926774, 6.0561604995516736434e-18),
        ),
    );

    s = ddadd2_vd2_vd2_vd(s, vmul_vd_vd_vd(vmul_vd_vd_vd(x2, vd2getx_vd_vd2(x)), t));

    let mut r = vadd_vd_vd_vd(vd2getx_vd_vd2(s), vd2gety_vd_vd2(s));

    #[cfg(not(target_feature = "avx512f"))]
    {
        r = vsel_vd_vo_vd_vd(vispinf_vo_vd(d), vcast_vd_d(f64::INFINITY), r);
        r = vsel_vd_vo_vd_vd(
            vor_vo_vo_vo(vlt_vo_vd_vd(d, vcast_vd_d(0.0)), visnan_vo_vd(d)),
            vcast_vd_d(f64::NAN),
            r,
        );
        r = vsel_vd_vo_vd_vd(
            veq_vo_vd_vd(d, vcast_vd_d(0.0)),
            vcast_vd_d(f64::NEG_INFINITY),
            r,
        );
    }

    #[cfg(target_feature = "avx512f")]
    {
        use crate::arch_simd::sleef::arch::helper_avx512::{vfixup_vd_vd_vd_vi2_i, vcast_vi2_i};
        r = vfixup_vd_vd_vd_vi2_i::<0>(
            r,
            d,
            vcast_vi2_i((4 << (2 * 4)) | (3 << (4 * 4)) | (5 << (5 * 4)) | (2 << (6 * 4))),
        );
    }

    r
}

#[inline(always)]
pub(crate) unsafe fn xlog1p(d: VDouble) -> VDouble {
    let dp1 = vadd_vd_vd_vd(d, vcast_vd_d(1.0));

    #[cfg(not(target_feature = "avx512f"))]
    let (m, s) = {
        let o = vlt_vo_vd_vd(dp1, vcast_vd_d(SLEEF_DBL_MIN));
        let dp1 = vsel_vd_vo_vd_vd(o, vmul_vd_vd_vd(dp1, vcast_vd_d(2_f64.powi(64))), dp1);
        let mut e = vilogb2k_vi_vd(vmul_vd_vd_vd(dp1, vcast_vd_d(1.0 / 0.75)));
        let t = vldexp3_vd_vd_vi(vcast_vd_d(1.0), vneg_vi_vi(e));
        let m = vmla_vd_vd_vd_vd(d, t, vsub_vd_vd_vd(t, vcast_vd_d(1.0)));
        e = vsel_vi_vo_vi_vi(vcast_vo32_vo64(o), vsub_vi_vi_vi(e, vcast_vi_i(64)), e);
        let s = ddmul_vd2_vd2_vd(
            vcast_vd2_d_d(0.693_147_180_559_945_3, 2.319_046_813_846_299_6e-17),
            vcast_vd_vi(e),
        );
        (m, s)
    };

    #[cfg(target_feature = "avx512f")]
    let (m, s) = {
        use crate::arch_simd::sleef::arch::helper_avx512::vgetexp_vd_vd;
        let mut e = vgetexp_vd_vd(vmul_vd_vd_vd(dp1, vcast_vd_d(1.0 / 0.75)));
        e = vsel_vd_vo_vd_vd(vispinf_vo_vd(e), vcast_vd_d(1024.0), e);
        let t = vldexp3_vd_vd_vi(vcast_vd_d(1.0), vneg_vi_vi(vrint_vi_vd(e)));
        let m = vmla_vd_vd_vd_vd(d, t, vsub_vd_vd_vd(t, vcast_vd_d(1.0)));
        let s = ddmul_vd2_vd2_vd(
            vcast_vd2_d_d(0.693147180559945286226764, 2.319046813846299558417771e-17),
            e,
        );
        (m, s)
    };

    let x: VDouble2 = dddiv_vd2_vd2_vd2(
        vcast_vd2_vd_vd(m, vcast_vd_d(0.0)),
        ddadd_vd2_vd_vd(vcast_vd_d(2.0), m),
    );
    let x2: VDouble = vmul_vd_vd_vd(vd2getx_vd_vd2(x), vd2getx_vd_vd2(x));

    let x4 = vmul_vd_vd_vd(x2, x2);
    let x8 = vmul_vd_vd_vd(x4, x4);
    let t: VDouble = poly7d(
        x2,
        x4,
        x8,
        1.532_076_988_502_701_4e-1,
        1.525_629_051_003_428_7e-1,
        1.818_605_932_937_786e-1,
        2.222_214_519_839_38e-1,
        2.857_142_932_794_299_3e-1,
        3.999_999_999_635_252e-1,
        6.666_666_666_667_334e-1,
    );

    let mut s = ddadd_vd2_vd2_vd2(s, ddscale_vd2_vd2_vd(x, vcast_vd_d(2.0)));
    s = ddadd_vd2_vd2_vd(s, vmul_vd_vd_vd(vmul_vd_vd_vd(x2, vd2getx_vd_vd2(x)), t));

    let mut r = vadd_vd_vd_vd(vd2getx_vd_vd2(s), vd2gety_vd_vd2(s));

    let ocore = vle_vo_vd_vd(d, vcast_vd_d(LOG1P_BOUND));
    if vtestallones_i_vo64(ocore) == 0 {
        r = vsel_vd_vo_vd_vd(ocore, r, xlog_u1(d));
    }

    r = vsel_vd_vo_vd_vd(
        vor_vo_vo_vo(vlt_vo_vd_vd(d, vcast_vd_d(-1.0)), visnan_vo_vd(d)),
        vcast_vd_d(f64::NAN),
        r,
    );
    r = vsel_vd_vo_vd_vd(
        veq_vo_vd_vd(d, vcast_vd_d(-1.0)),
        vcast_vd_d(f64::NEG_INFINITY),
        r,
    );
    r = vsel_vd_vo_vd_vd(visnegzero_vo_vd(d), vcast_vd_d(-0.0), r);

    r
}

#[inline(always)]
pub(crate) unsafe fn xsqrt_u05(d: VDouble) -> VDouble {
    #[cfg(target_feature = "fma")]
    {
        use helper::{vfma_vd_vd_vd_vd, vfmanp_vd_vd_vd_vd, vfmapn_vd_vd_vd_vd};
        let q: VDouble;
        let mut w: VDouble;
        let mut x: VDouble;
        let mut y: VDouble;
        let mut z: VDouble;

        let mut d = vsel_vd_vo_vd_vd(vlt_vo_vd_vd(d, vcast_vd_d(0.0)), vcast_vd_d(f64::NAN), d);

        let o = vlt_vo_vd_vd(d, vcast_vd_d(8.636168555094445E-78));
        d = vsel_vd_vo_vd_vd(o, vmul_vd_vd_vd(d, vcast_vd_d(1.157920892373162E77)), d);
        q = vsel_vd_vo_vd_vd(o, vcast_vd_d(2.9387358770557188E-39), vcast_vd_d(1.0));

        y = vreinterpret_vd_vm(vsub64_vm_vm_vm(
            vcast_vm_i_i(0x5fe6ec85, 0xe7de30dau32 as i32),
            vsrl64_vm_vm_i::<1>(vreinterpret_vm_vd(d)),
        ));

        x = vmul_vd_vd_vd(d, y);
        w = vmul_vd_vd_vd(vcast_vd_d(0.5), y);
        y = vfmanp_vd_vd_vd_vd(x, w, vcast_vd_d(0.5));

        x = vfma_vd_vd_vd_vd(x, y, x);
        w = vfma_vd_vd_vd_vd(w, y, w);
        y = vfmanp_vd_vd_vd_vd(x, w, vcast_vd_d(0.5));

        x = vfma_vd_vd_vd_vd(x, y, x);
        w = vfma_vd_vd_vd_vd(w, y, w);
        y = vfmanp_vd_vd_vd_vd(x, w, vcast_vd_d(0.5));

        x = vfma_vd_vd_vd_vd(x, y, x);
        w = vfma_vd_vd_vd_vd(w, y, w);

        y = vfmanp_vd_vd_vd_vd(x, w, vcast_vd_d(1.5));
        w = vadd_vd_vd_vd(w, w);
        w = vmul_vd_vd_vd(w, y);
        x = vmul_vd_vd_vd(w, d);
        y = vfmapn_vd_vd_vd_vd(w, d, x);
        z = vfmanp_vd_vd_vd_vd(w, x, vcast_vd_d(1.0));

        z = vfmanp_vd_vd_vd_vd(w, y, z);
        w = vmul_vd_vd_vd(vcast_vd_d(0.5), x);
        w = vfma_vd_vd_vd_vd(w, z, y);
        w = vadd_vd_vd_vd(w, x);

        w = vmul_vd_vd_vd(w, q);

        w = vsel_vd_vo_vd_vd(
            vor_vo_vo_vo(
                veq_vo_vd_vd(d, vcast_vd_d(0.0)),
                veq_vo_vd_vd(d, vcast_vd_d(f64::INFINITY)),
            ),
            d,
            w,
        );

        w = vsel_vd_vo_vd_vd(vlt_vo_vd_vd(d, vcast_vd_d(0.0)), vcast_vd_d(f64::NAN), w);

        w
    }

    #[cfg(not(target_feature = "fma"))]
    {
        use crate::sleef_types::VMask;
        let mut q: VDouble;
        let mut o: VMask;

        let mut d = vsel_vd_vo_vd_vd(vlt_vo_vd_vd(d, vcast_vd_d(0.0)), vcast_vd_d(f64::NAN), d);

        o = vlt_vo_vd_vd(d, vcast_vd_d(8.636168555094445E-78));
        d = vsel_vd_vo_vd_vd(o, vmul_vd_vd_vd(d, vcast_vd_d(1.157920892373162E77)), d);
        q = vsel_vd_vo_vd_vd(
            o,
            vcast_vd_d(2.938_735_877_055_719E-39 * 0.5),
            vcast_vd_d(0.5),
        );

        o = vgt_vo_vd_vd(d, vcast_vd_d(1.340_780_792_994_259_7e154));
        d = vsel_vd_vo_vd_vd(
            o,
            vmul_vd_vd_vd(d, vcast_vd_d(7.458_340_731_200_207e-155)),
            d,
        );
        q = vsel_vd_vo_vd_vd(o, vcast_vd_d(1.157_920_892_373_162e77 * 0.5), q);

        let mut x = vreinterpret_vd_vm(vsub64_vm_vm_vm(
            vcast_vm_i_i(0x5fe6ec86, 0),
            vsrl64_vm_vm_i::<1>(vreinterpret_vm_vd(vadd_vd_vd_vd(d, vcast_vd_d(1e-320)))),
        ));

        x = vmul_vd_vd_vd(
            x,
            vsub_vd_vd_vd(
                vcast_vd_d(1.5),
                vmul_vd_vd_vd(vmul_vd_vd_vd(vmul_vd_vd_vd(vcast_vd_d(0.5), d), x), x),
            ),
        );
        x = vmul_vd_vd_vd(
            x,
            vsub_vd_vd_vd(
                vcast_vd_d(1.5),
                vmul_vd_vd_vd(vmul_vd_vd_vd(vmul_vd_vd_vd(vcast_vd_d(0.5), d), x), x),
            ),
        );
        x = vmul_vd_vd_vd(
            x,
            vsub_vd_vd_vd(
                vcast_vd_d(1.5),
                vmul_vd_vd_vd(vmul_vd_vd_vd(vmul_vd_vd_vd(vcast_vd_d(0.5), d), x), x),
            ),
        );
        x = vmul_vd_vd_vd(x, d);

        let d2 = ddmul_vd2_vd2_vd2(ddadd2_vd2_vd_vd2(d, ddmul_vd2_vd_vd(x, x)), ddrec_vd2_vd(x));

        x = vmul_vd_vd_vd(vadd_vd_vd_vd(vd2getx_vd_vd2(d2), vd2gety_vd_vd2(d2)), q);

        x = vsel_vd_vo_vd_vd(vispinf_vo_vd(d), vcast_vd_d(f64::INFINITY), x);
        x = vsel_vd_vo_vd_vd(veq_vo_vd_vd(d, vcast_vd_d(0.0)), d, x);

        x
    }
}

#[inline(always)]
unsafe fn ddmla_vd2_vd_vd2_vd2(x: VDouble, y: VDouble2, z: VDouble2) -> VDouble2 {
    let mul = ddmul_vd2_vd2_vd(y, x);

    ddadd_vd2_vd2_vd2(z, mul)
}

#[inline(always)]
unsafe fn poly2dd_b(x: VDouble, c1: VDouble2, c0: VDouble2) -> VDouble2 {
    ddmla_vd2_vd_vd2_vd2(x, c1, c0)
}

#[inline(always)]
unsafe fn poly2dd(x: VDouble, c1: VDouble, c0: VDouble2) -> VDouble2 {
    let c1_dd = vcast_vd2_vd_vd(c1, vcast_vd_d(0.0));

    ddmla_vd2_vd_vd2_vd2(x, c1_dd, c0)
}

#[inline(always)]
unsafe fn poly4dd(x: VDouble, c3: VDouble, c2: VDouble2, c1: VDouble2, c0: VDouble2) -> VDouble2 {
    let x2 = vmul_vd_vd_vd(x, x);

    let high = poly2dd(x, c3, c2);

    let low = poly2dd_b(x, c1, c0);

    ddmla_vd2_vd_vd2_vd2(x2, high, low)
}

#[inline(always)]
pub(crate) unsafe fn xerf_u1(a: VDouble) -> VDouble {
    let t: VDouble;
    let x = vabs_vd_vd(a);
    let mut t2: VDouble2;
    let x2 = vmul_vd_vd_vd(x, x);
    let x4 = vmul_vd_vd_vd(x2, x2);
    let x8 = vmul_vd_vd_vd(x4, x4);
    let x16 = vmul_vd_vd_vd(x8, x8);
    let o25 = vle_vo_vd_vd(x, vcast_vd_d(2.5));

    if vtestallones_i_vo64(o25) != 0 {
        t = poly21d(
            x,
            x2,
            x4,
            x8,
            x16,
            -2.083_271_002_525_222e-15,
            7.151_909_970_790_897e-14,
            -1.162_238_220_110_999_4e-12,
            1.186_474_230_821_585_3e-11,
            -8.499_973_178_354_613e-11,
            4.507_647_462_598_841_6e-10,
            -1.808_044_474_288_849e-9,
            5.435_081_826_716_212e-9,
            -1.143_939_895_758_628_5e-8,
            1.215_442_362_680_889_2e-8,
            1.669_878_756_181_250_4e-8,
            -9.808_074_602_255_194e-8,
            1.389_000_557_865_837_2e-7,
            2.945_514_529_987_332e-7,
            -1.842_918_273_003_998_3e-6,
            3.417_987_836_115_362e-6,
            3.860_236_356_493_129e-6,
            -3.309_403_072_749_947_5e-5,
            1.060_862_922_597_579_5e-4,
            2.323_253_155_213_076_2e-4,
            1.490_149_719_145_544_7e-4,
        );

        t2 = poly4dd(
            x,
            t,
            vcast_vd2_d_d(0.009_287_795_839_227_56, 7.928_755_946_396_111e-19),
            vcast_vd2_d_d(0.042_275_531_758_784_69, 1.378_522_662_050_101_5e-19),
            vcast_vd2_d_d(0.070_523_697_943_469_53, 9.584_662_807_079_21e-19),
        );

        t2 = ddadd_vd2_vd_vd2(vcast_vd_d(1.0), ddmul_vd2_vd2_vd(t2, x));
        t2 = ddsqu_vd2_vd2(t2);
        t2 = ddsqu_vd2_vd2(t2);
        t2 = ddsqu_vd2_vd2(t2);
        t2 = ddsqu_vd2_vd2(t2);
        t2 = ddrec_vd2_vd2(t2);
    } else {
        t = poly21d_(
            x,
            x2,
            x4,
            x8,
            x16,
            vsel_vd_vo_d_d(o25, -2.083_271_002_525_222e-15, -4.024_015_130_752_622e-19),
            vsel_vd_vo_d_d(o25, 7.151_909_970_790_897e-14, 3.847_193_332_817_048e-17),
            vsel_vd_vo_d_d(
                o25,
                -1.162_238_220_110_999_4e-12,
                -1.749_316_241_455_644e-15,
            ),
            vsel_vd_vo_d_d(o25, 1.186_474_230_821_585_3e-11, 5.029_618_322_872_873e-14),
            vsel_vd_vo_d_d(
                o25,
                -8.499_973_178_354_613e-11,
                -1.025_221_466_851_463_2e-12,
            ),
            vsel_vd_vo_d_d(
                o25,
                4.507_647_462_598_841_6e-10,
                1.573_695_559_331_945_6e-11,
            ),
            vsel_vd_vo_d_d(o25, -1.808_044_474_288_849e-9, -1.884_658_558_040_203_7e-10),
            vsel_vd_vo_d_d(o25, 5.435_081_826_716_212e-9, 1.798_167_853_032_159_3e-9),
            vsel_vd_vo_d_d(
                o25,
                -1.143_939_895_758_628_5e-8,
                -1.380_745_342_355_033_1e-8,
            ),
            vsel_vd_vo_d_d(o25, 1.215_442_362_680_889_2e-8, 8.525_705_726_469_103e-8),
            vsel_vd_vo_d_d(o25, 1.669_878_756_181_250_4e-8, -4.160_448_058_101_303_4e-7),
            vsel_vd_vo_d_d(o25, -9.808_074_602_255_194e-8, 1.517_272_660_008_588_5e-6),
            vsel_vd_vo_d_d(o25, 1.389_000_557_865_837_2e-7, -3.341_634_127_317_201_7e-6),
            vsel_vd_vo_d_d(o25, 2.945_514_529_987_332e-7, -2.515_023_395_879_724_5e-6),
            vsel_vd_vo_d_d(o25, -1.842_918_273_003_998_3e-6, 6.539_731_269_664_908e-5),
            vsel_vd_vo_d_d(o25, 3.417_987_836_115_362e-6, -3.551_065_097_428_388_7e-4),
            vsel_vd_vo_d_d(o25, 3.860_236_356_493_129e-6, 1.210_736_097_958_368_9e-3),
            vsel_vd_vo_d_d(
                o25,
                -3.309_403_072_749_947_5e-5,
                -2.605_566_912_579_998_7e-3,
            ),
            vsel_vd_vo_d_d(o25, 1.060_862_922_597_579_5e-4, 1.252_823_202_436_093_2e-3),
            vsel_vd_vo_d_d(o25, 2.323_253_155_213_076_2e-4, 1.820_191_395_263_313_2e-2),
            vsel_vd_vo_d_d(o25, 1.490_149_719_145_544_7e-4, -1.021_557_155_453_466e-1),
        );

        t2 = poly4dd(
            x,
            t,
            vsel_vd2_vo_vd2_vd2(
                o25,
                vcast_vd2_d_d(0.009_287_795_839_227_56, 7.928_755_946_396_111e-19),
                vcast_vd2_d_d(-0.636_910_443_836_417_5, -2.424_947_752_653_943_3e-17),
            ),
            vsel_vd2_vo_vd2_vd2(
                o25,
                vcast_vd2_d_d(0.042_275_531_758_784_69, 1.378_522_662_050_101_5e-19),
                vcast_vd2_d_d(-1.128_292_606_180_396_2, -6.297_033_886_041_1e-17),
            ),
            vsel_vd2_vo_vd2_vd2(
                o25,
                vcast_vd2_d_d(0.070_523_697_943_469_53, 9.584_662_807_079_21e-19),
                vcast_vd2_d_d(-1.226_131_378_518_480_5e-5, -5.532_970_751_449_011e-22),
            ),
        );

        let mut s2 = ddadd_vd2_vd_vd2(vcast_vd_d(1.0), ddmul_vd2_vd2_vd(t2, x));
        s2 = ddsqu_vd2_vd2(s2);
        s2 = ddsqu_vd2_vd2(s2);
        s2 = ddsqu_vd2_vd2(s2);
        s2 = ddsqu_vd2_vd2(s2);
        s2 = ddrec_vd2_vd2(s2);
        t2 = vsel_vd2_vo_vd2_vd2(o25, s2, vcast_vd2_vd_vd(expk(t2), vcast_vd_d(0.0)));
    }

    t2 = ddadd2_vd2_vd2_vd(t2, vcast_vd_d(-1.0));

    let mut z = vneg_vd_vd(vadd_vd_vd_vd(vd2getx_vd_vd2(t2), vd2gety_vd_vd2(t2)));
    z = vsel_vd_vo_vd_vd(
        vlt_vo_vd_vd(x, vcast_vd_d(1e-8)),
        vmul_vd_vd_vd(x, vcast_vd_d(1.128_379_167_095_512_6)),
        z,
    );
    z = vsel_vd_vo_vd_vd(vge_vo_vd_vd(x, vcast_vd_d(6.0)), vcast_vd_d(1.0), z);
    z = vsel_vd_vo_vd_vd(visinf_vo_vd(a), vcast_vd_d(1.0), z);
    z = vsel_vd_vo_vd_vd(veq_vo_vd_vd(a, vcast_vd_d(0.0)), vcast_vd_d(0.0), z);
    z = vmulsign_vd_vd_vd(z, a);

    z
}

#[inline(always)]
pub(crate) unsafe fn xhypot_u05(x: VDouble, y: VDouble) -> VDouble {
    let x = vabs_vd_vd(x);
    let y = vabs_vd_vd(y);

    let min = vmin_vd_vd_vd(x, y);
    let mut n = min;
    let max = vmax_vd_vd_vd(x, y);
    let mut d = max;

    let o = vlt_vo_vd_vd(max, vcast_vd_d(SLEEF_DBL_MIN));
    n = vsel_vd_vo_vd_vd(o, vmul_vd_vd_vd(n, vcast_vd_d(2_f64.powi(54))), n);
    d = vsel_vd_vo_vd_vd(o, vmul_vd_vd_vd(d, vcast_vd_d(2_f64.powi(54))), d);

    let t = dddiv_vd2_vd2_vd2(
        vcast_vd2_vd_vd(n, vcast_vd_d(0.0)),
        vcast_vd2_vd_vd(d, vcast_vd_d(0.0)),
    );
    let t = ddmul_vd2_vd2_vd(
        ddsqrt_vd2_vd2(ddadd2_vd2_vd2_vd(ddsqu_vd2_vd2(t), vcast_vd_d(1.0))),
        max,
    );

    let mut ret = vadd_vd_vd_vd(vd2getx_vd_vd2(t), vd2gety_vd_vd2(t));

    ret = vsel_vd_vo_vd_vd(visnan_vo_vd(ret), vcast_vd_d(f64::INFINITY), ret);
    ret = vsel_vd_vo_vd_vd(veq_vo_vd_vd(min, vcast_vd_d(0.0)), max, ret);
    ret = vsel_vd_vo_vd_vd(
        vor_vo_vo_vo(visnan_vo_vd(x), visnan_vo_vd(y)),
        vcast_vd_d(f64::NAN),
        ret,
    );
    ret = vsel_vd_vo_vd_vd(
        vor_vo_vo_vo(
            veq_vo_vd_vd(x, vcast_vd_d(f64::INFINITY)),
            veq_vo_vd_vd(y, vcast_vd_d(f64::INFINITY)),
        ),
        vcast_vd_d(f64::INFINITY),
        ret,
    );

    ret
}

#[inline(always)]
pub(crate) unsafe fn vfloor2_vd_vd(x: VDouble) -> VDouble {
    let mut fr = vsub_vd_vd_vd(
        x,
        vmul_vd_vd_vd(
            vcast_vd_d((1i64 << 31) as f64),
            vcast_vd_vi(vtruncate_vi_vd(vmul_vd_vd_vd(
                x,
                vcast_vd_d(1.0 / (1i64 << 31) as f64),
            ))),
        ),
    );
    fr = vsub_vd_vd_vd(fr, vcast_vd_vi(vtruncate_vi_vd(fr)));
    fr = vsel_vd_vo_vd_vd(
        vlt_vo_vd_vd(fr, vcast_vd_d(0.0)),
        vadd_vd_vd_vd(fr, vcast_vd_d(1.0)),
        fr,
    );
    vsel_vd_vo_vd_vd(
        vor_vo_vo_vo(
            visinf_vo_vd(x),
            vge_vo_vd_vd(vabs_vd_vd(x), vcast_vd_d((1i64 << 52) as f64)),
        ),
        x,
        vcopysign_vd_vd_vd(vsub_vd_vd_vd(x, fr), x),
    )
}

#[inline(always)]
pub(crate) unsafe fn xfloor(x: VDouble) -> VDouble {
    vfloor2_vd_vd(x)
}

#[inline(always)]
pub(crate) unsafe fn vceil2_vd_vd(x: VDouble) -> VDouble {
    let mut fr = vsub_vd_vd_vd(
        x,
        vmul_vd_vd_vd(
            vcast_vd_d((1i64 << 31) as f64),
            vcast_vd_vi(vtruncate_vi_vd(vmul_vd_vd_vd(
                x,
                vcast_vd_d(1.0 / (1i64 << 31) as f64),
            ))),
        ),
    );
    fr = vsub_vd_vd_vd(fr, vcast_vd_vi(vtruncate_vi_vd(fr)));
    fr = vsel_vd_vo_vd_vd(
        vle_vo_vd_vd(fr, vcast_vd_d(0.0)),
        fr,
        vsub_vd_vd_vd(fr, vcast_vd_d(1.0)),
    );
    vsel_vd_vo_vd_vd(
        vor_vo_vo_vo(
            visinf_vo_vd(x),
            vge_vo_vd_vd(vabs_vd_vd(x), vcast_vd_d((1i64 << 52) as f64)),
        ),
        x,
        vcopysign_vd_vd_vd(vsub_vd_vd_vd(x, fr), x),
    )
}

#[inline(always)]
pub(crate) unsafe fn xceil(x: VDouble) -> VDouble {
    vceil2_vd_vd(x)
}

#[allow(dead_code)]
#[must_use]
#[inline(always)]
pub(crate) unsafe fn xcopysign(x: VDouble, y: VDouble) -> VDouble {
    vcopysign_vd_vd_vd(x, y)
}

#[inline(always)]
pub(crate) unsafe fn xtrunc(x: VDouble) -> VDouble {
    vtruncate2_vd_vd(x)
}

#[inline(always)]
pub(crate) unsafe fn xround(x: VDouble) -> VDouble {
    vround2_vd_vd(x)
}

#[inline(always)]
pub(crate) unsafe fn xfmax(x: VDouble, y: VDouble) -> VDouble {
    vsel_vd_vo_vd_vd(
        visnan_vo_vd(y),
        x,
        vsel_vd_vo_vd_vd(vgt_vo_vd_vd(x, y), x, y),
    )
}

#[inline(always)]
pub(crate) unsafe fn xfmin(x: VDouble, y: VDouble) -> VDouble {
    vsel_vd_vo_vd_vd(
        visnan_vo_vd(y),
        x,
        vsel_vd_vo_vd_vd(vgt_vo_vd_vd(y, x), x, y),
    )
}
