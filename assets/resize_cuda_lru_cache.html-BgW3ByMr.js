import{_ as n,c as a,d as e,o as c}from"./app-CZ_qsLqA.js";const i={};function t(l,s){return c(),a("div",null,s[0]||(s[0]=[e(`<h1>resize_cuda_lru_cache</h1><div class="language-rust line-numbers-mode" data-highlighter="prismjs" data-ext="rs"><pre><code><span class="line"><span class="token function">resize_cuda_lru_cache</span><span class="token punctuation">(</span>new_size<span class="token punctuation">:</span> <span class="token keyword">usize</span><span class="token punctuation">,</span> device_id<span class="token punctuation">:</span> <span class="token keyword">usize</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>Adjusts the size of the memory cache used by the GPU allocator. This function allows dynamic control over memory caching behavior.</p><h2>Parameters:</h2><ul><li><code>new_size</code>: <code>usize</code><ul><li>New capacity of the LRU cache</li><li>Must be a non-negative integer</li><li>Determines how many memory allocations can be cached</li></ul></li><li><code>device_id</code>: <code>usize</code><ul><li>ID of the GPU device</li><li>Usually 0 for single GPU</li></ul></li></ul><h2>Behavior</h2><ul><li>When <code>new_size</code> &gt;= current size: <ul><li>Cache capacity increases</li><li>Existing cached memory is preserved</li></ul></li><li>When <code>new_size</code> &lt; current size: <ul><li>All cached memory is deallocated</li><li>Cache is resized to new capacity</li></ul></li></ul><h2>Examples</h2><div class="language-rust line-numbers-mode" data-highlighter="prismjs" data-ext="rs"><pre><code><span class="line"><span class="token keyword">use</span> <span class="token namespace">hpt<span class="token punctuation">::</span></span><span class="token punctuation">{</span><span class="token namespace">error<span class="token punctuation">::</span></span><span class="token class-name">TensorError</span><span class="token punctuation">,</span> <span class="token namespace">utils<span class="token punctuation">::</span></span>resize_cuda_lru_cache<span class="token punctuation">}</span><span class="token punctuation">;</span></span>
<span class="line"></span>
<span class="line"><span class="token keyword">fn</span> <span class="token function-definition function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">-&gt;</span> <span class="token class-name">Result</span><span class="token operator">&lt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token class-name">TensorError</span><span class="token operator">&gt;</span> <span class="token punctuation">{</span></span>
<span class="line">    <span class="token comment">// Increase cache size to 1000 entries</span></span>
<span class="line">    <span class="token function">resize_cuda_lru_cache</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line">    </span>
<span class="line">    <span class="token comment">// Reduce cache size to free memory</span></span>
<span class="line">    <span class="token function">resize_cuda_lru_cache</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line">    </span>
<span class="line">    <span class="token class-name">Ok</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token punctuation">}</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,9)]))}const o=n(i,[["render",t]]),u=JSON.parse('{"path":"/user_guide/utils/resize_cuda_lru_cache.html","title":"resize_cuda_lru_cache","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1745030262000,"contributors":[{"name":"Jianqoq","username":"Jianqoq","email":"120760306+Jianqoq@users.noreply.github.com","commits":1,"url":"https://github.com/Jianqoq"}],"changelog":[{"hash":"2070c4e5cc505acf63902dfed1ccaee672a87e43","time":1745030262000,"email":"120760306+Jianqoq@users.noreply.github.com","author":"Jianqoq","message":"Merge pull request #180 from Jianqoq/Jianqoq"}]},"filePathRelative":"user_guide/utils/resize_cuda_lru_cache.md"}');export{o as comp,u as data};
