import{_ as o,c as t,a as i,o as s}from"./app-CwUDEbfe.js";const r={};function a(n,e){return s(),t("div",null,e[0]||(e[0]=[i("<h1>Adding New Arch Support</h1><h3>Things to know</h3><p>If you found that the CPU you are using has the Simd instruction Hpt is not supported and you are willing to add support for that instruction like <code>avx512f</code>.</p><h3>How</h3><ol><li><p>All the Tensor operations are using implementation from <code>hpt-types/scalars/</code> and <code>hpt-types/vectors/arch_simd</code>.</p></li><li><p>The folder <code>vectors</code> defines the simd vectors for simd registers with different size of bits. If your simd register is using <code>128-bit</code> and the instruction is not supported. You will want to go to <code>arch_simd/_128bit</code> folder, create new folder for the new arch.</p></li><li><p>The folder <code>scalars</code> defines the scalar computations. Most of the time you don&#39;t need to do anything on this folder unless for some scalar computation, your CPU has special instruction to perform that computation.</p></li><li><p>The file <code>convertion.rs</code> defines the type conversion traits and uses proc_macro defined from <code>hpt-macros</code> crate to auto implement the type conversion. If your CPU has special conversion instruction, you may want to go to the <code>hpt-macros</code> and modify the macro code. The file <code>into_scalar.rs</code> is just using the method from <code>convertion.rs</code>, you won&#39;t need to worry about this file</p></li><li><p>After you finished all the steps above, you can start to run the hpt-tests crate and make sure all the tests passed.</p></li><li><p>Make a pull request.</p></li></ol>",5)]))}const d=o(r,[["render",a]]),l=JSON.parse('{"path":"/dev_guide/adding_new_arch.html","title":"Adding New Arch Support","lang":"zh-CN","frontmatter":{},"git":{"updatedTime":1750800187000,"contributors":[{"name":"Jianqoq","username":"Jianqoq","email":"120760306+Jianqoq@users.noreply.github.com","commits":1,"url":"https://github.com/Jianqoq"}],"changelog":[{"hash":"79305ab7fe2dbd0c1213b2f6140969d78a941c13","time":1750800187000,"email":"120760306+Jianqoq@users.noreply.github.com","author":"Jianqoq","message":"Merge pull request #183 from Jianqoq/182-optimize-matmul-for-special-shape"}]},"filePathRelative":"dev_guide/adding_new_arch.md"}');export{d as comp,l as data};
